{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "429ed4fb-c368-47d9-804f-decc81629592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Root directory: /home/jeanhuit/Documents/Workspace/simulation/results/\n",
      "  Output directory: /home/jeanhuit/Documents/Workspace/simulation/output\n",
      "  Figure directory: /home/jeanhuit/Documents/Workspace/simulation/figures\n",
      "  Window size: 5s\n",
      "  Scenarios: ['highway', 'mixed', 'urban']\n",
      "  Densities: [50, 100, 150]\n",
      "Initializing data loader...\n",
      "\n",
      "============================================================\n",
      "LOADING ALL SCENARIOS AND DENSITIES\n",
      "============================================================\n",
      "\n",
      "Loading highway (density=50, run=run-1)...\n",
      "  Loaded: bsm_log.csv (14500 rows)\n",
      "  Loaded: rssi_log.csv (49 rows)\n",
      "  Loaded: neighbor_log.csv (7250 rows)\n",
      "  Loaded: sybil_log.csv (80 rows)\n",
      "  Loaded: replay_log.csv (5 rows)\n",
      "  Loaded: jammer_log.csv (5000 rows)\n",
      "\n",
      "Loading highway (density=100, run=run-1)...\n",
      "  Loaded: bsm_log.csv (29000 rows)\n",
      "  Loaded: rssi_log.csv (75 rows)\n",
      "  Loaded: neighbor_log.csv (14500 rows)\n",
      "  Loaded: sybil_log.csv (80 rows)\n",
      "  Loaded: replay_log.csv (5 rows)\n",
      "  Loaded: jammer_log.csv (5000 rows)\n",
      "\n",
      "Loading highway (density=150, run=run-1)...\n",
      "  Loaded: bsm_log.csv (43500 rows)\n",
      "  Loaded: rssi_log.csv (74 rows)\n",
      "  Loaded: neighbor_log.csv (21750 rows)\n",
      "  Loaded: sybil_log.csv (80 rows)\n",
      "  Loaded: replay_log.csv (5 rows)\n",
      "  Loaded: jammer_log.csv (5000 rows)\n",
      "\n",
      "Loading mixed (density=50, run=run-1)...\n",
      "  Loaded: bsm_log.csv (14500 rows)\n",
      "  Loaded: rssi_log.csv (46 rows)\n",
      "  Loaded: neighbor_log.csv (7250 rows)\n",
      "  Loaded: sybil_log.csv (80 rows)\n",
      "  Loaded: replay_log.csv (5 rows)\n",
      "  Loaded: jammer_log.csv (5000 rows)\n",
      "\n",
      "Loading mixed (density=100, run=run-1)...\n",
      "  Loaded: bsm_log.csv (29000 rows)\n",
      "  Loaded: rssi_log.csv (95 rows)\n",
      "  Loaded: neighbor_log.csv (14500 rows)\n",
      "  Loaded: sybil_log.csv (80 rows)\n",
      "  Loaded: replay_log.csv (5 rows)\n",
      "  Loaded: jammer_log.csv (5000 rows)\n",
      "\n",
      "Loading mixed (density=150, run=run-1)...\n",
      "  Loaded: bsm_log.csv (43500 rows)\n",
      "  Loaded: rssi_log.csv (137 rows)\n",
      "  Loaded: neighbor_log.csv (21750 rows)\n",
      "  Loaded: sybil_log.csv (80 rows)\n",
      "  Loaded: replay_log.csv (5 rows)\n",
      "  Loaded: jammer_log.csv (5000 rows)\n",
      "\n",
      "Loading urban (density=50, run=run-1)...\n",
      "  Loaded: bsm_log.csv (14500 rows)\n",
      "  Loaded: rssi_log.csv (14 rows)\n",
      "  Loaded: neighbor_log.csv (7250 rows)\n",
      "  Loaded: sybil_log.csv (80 rows)\n",
      "  Loaded: replay_log.csv (5 rows)\n",
      "  Loaded: jammer_log.csv (5000 rows)\n",
      "\n",
      "Loading urban (density=100, run=run-1)...\n",
      "  Loaded: bsm_log.csv (29000 rows)\n",
      "  Loaded: rssi_log.csv (14 rows)\n",
      "  Loaded: neighbor_log.csv (14500 rows)\n",
      "  Loaded: sybil_log.csv (80 rows)\n",
      "  Loaded: replay_log.csv (5 rows)\n",
      "  Loaded: jammer_log.csv (5000 rows)\n",
      "\n",
      "Loading urban (density=150, run=run-1)...\n",
      "  Loaded: bsm_log.csv (43500 rows)\n",
      "  Loaded: rssi_log.csv (36 rows)\n",
      "  Loaded: neighbor_log.csv (21750 rows)\n",
      "  Loaded: sybil_log.csv (80 rows)\n",
      "  Loaded: replay_log.csv (5 rows)\n",
      "  Loaded: jammer_log.csv (5000 rows)\n",
      "\n",
      "Loaded 9 scenario-density-run combinations:\n",
      "  - highway_50_run-1\n",
      "  - highway_100_run-1\n",
      "  - highway_150_run-1\n",
      "  - mixed_50_run-1\n",
      "  - mixed_100_run-1\n",
      "  - mixed_150_run-1\n",
      "  - urban_50_run-1\n",
      "  - urban_100_run-1\n",
      "  - urban_150_run-1\n",
      "\n",
      "============================================================\n",
      "NETWORK STATISTICS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Consolidated Statistics:\n",
      "scenario  density   run  bsm_sent  bsm_unique_senders  bsm_received  pdr  rssi_mean  rssi_std  rssi_min  rssi_max  unique_vehicles  sybil_events  replay_events  jammer_events  total_attack_events  unique_attackers\n",
      " highway       50 run-1     14500                   0             0    0       -1.0       0.0        -1        -1                0            80              5           5000                 5085                 0\n",
      " highway      100 run-1     29000                   0             0    0       -1.0       0.0        -1        -1                0            80              5           5000                 5085                 0\n",
      " highway      150 run-1     43500                   0             0    0       -1.0       0.0        -1        -1                0            80              5           5000                 5085                 0\n",
      "   mixed       50 run-1     14500                   0             0    0       -1.0       0.0        -1        -1                0            80              5           5000                 5085                 0\n",
      "   mixed      100 run-1     29000                   0             0    0       -1.0       0.0        -1        -1                0            80              5           5000                 5085                 0\n",
      "   mixed      150 run-1     43500                   0             0    0       -1.0       0.0        -1        -1                0            80              5           5000                 5085                 0\n",
      "   urban       50 run-1     14500                   0             0    0       -1.0       0.0        -1        -1                0            80              5           5000                 5085                 0\n",
      "   urban      100 run-1     29000                   0             0    0       -1.0       0.0        -1        -1                0            80              5           5000                 5085                 0\n",
      "   urban      150 run-1     43500                   0             0    0       -1.0       0.0        -1        -1                0            80              5           5000                 5085                 0\n",
      "  Data saved: /home/jeanhuit/Documents/Workspace/simulation/output/consolidated_network_stats.csv (9 rows)\n",
      "  LaTeX table saved: /home/jeanhuit/Documents/Workspace/simulation/output/network_stats_latex.tex\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) ['avg_neighbors'] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 393\u001b[39m\n\u001b[32m    385\u001b[39m export_latex_table(\n\u001b[32m    386\u001b[39m     consolidated_stats,\n\u001b[32m    387\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnetwork_stats_latex.tex\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    388\u001b[39m     caption=\u001b[33m\"\u001b[39m\u001b[33mNetwork Statistics for All Scenarios and Densities\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    389\u001b[39m     label=\u001b[33m\"\u001b[39m\u001b[33mtab:network_stats\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    390\u001b[39m )\n\u001b[32m    392\u001b[39m \u001b[38;5;66;03m# Create summary by scenario\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m scenario_summary = \u001b[43mconsolidated_stats\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mscenario\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbsm_sent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbsm_received\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpdr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mavg_neighbors\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtotal_attack_events\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43munique_attackers\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m    400\u001b[39m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m.round(\u001b[32m2\u001b[39m)\n\u001b[32m    402\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mScenario-wise Summary:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    403\u001b[39m \u001b[38;5;28mprint\u001b[39m(scenario_summary)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workspace/Env/ml/lib/python3.13/site-packages/pandas/core/groupby/generic.py:1432\u001b[39m, in \u001b[36mDataFrameGroupBy.aggregate\u001b[39m\u001b[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m   1429\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine_kwargs\u001b[39m\u001b[33m\"\u001b[39m] = engine_kwargs\n\u001b[32m   1431\u001b[39m op = GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args=args, kwargs=kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1432\u001b[39m result = \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1434\u001b[39m     \u001b[38;5;66;03m# GH #52849\u001b[39;00m\n\u001b[32m   1435\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.as_index \u001b[38;5;129;01mand\u001b[39;00m is_list_like(func):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workspace/Env/ml/lib/python3.13/site-packages/pandas/core/apply.py:190\u001b[39m, in \u001b[36mApply.agg\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_str()\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(func):\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[32m    192\u001b[39m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agg_list_like()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workspace/Env/ml/lib/python3.13/site-packages/pandas/core/apply.py:423\u001b[39m, in \u001b[36mApply.agg_dict_like\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magg_dict_like\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> DataFrame | Series:\n\u001b[32m    416\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[33;03m    Compute aggregation in the case of a dict-like argument.\u001b[39;00m\n\u001b[32m    418\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    421\u001b[39m \u001b[33;03m    Result of aggregation.\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magg_or_apply_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43magg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workspace/Env/ml/lib/python3.13/site-packages/pandas/core/apply.py:1603\u001b[39m, in \u001b[36mGroupByApply.agg_or_apply_dict_like\u001b[39m\u001b[34m(self, op_name)\u001b[39m\n\u001b[32m   1598\u001b[39m     kwargs.update({\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m: engine, \u001b[33m\"\u001b[39m\u001b[33mengine_kwargs\u001b[39m\u001b[33m\"\u001b[39m: engine_kwargs})\n\u001b[32m   1600\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m com.temp_setattr(\n\u001b[32m   1601\u001b[39m     obj, \u001b[33m\"\u001b[39m\u001b[33mas_index\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition=\u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[33m\"\u001b[39m\u001b[33mas_index\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1602\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1603\u001b[39m     result_index, result_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_dict_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1606\u001b[39m result = \u001b[38;5;28mself\u001b[39m.wrap_results_dict_like(selected_obj, result_index, result_data)\n\u001b[32m   1607\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workspace/Env/ml/lib/python3.13/site-packages/pandas/core/apply.py:462\u001b[39m, in \u001b[36mApply.compute_dict_like\u001b[39m\u001b[34m(self, op_name, selected_obj, selection, kwargs)\u001b[39m\n\u001b[32m    460\u001b[39m is_groupby = \u001b[38;5;28misinstance\u001b[39m(obj, (DataFrameGroupBy, SeriesGroupBy))\n\u001b[32m    461\u001b[39m func = cast(AggFuncTypeDict, \u001b[38;5;28mself\u001b[39m.func)\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m func = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalize_dictlike_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m is_non_unique_col = (\n\u001b[32m    465\u001b[39m     selected_obj.ndim == \u001b[32m2\u001b[39m\n\u001b[32m    466\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m selected_obj.columns.nunique() < \u001b[38;5;28mlen\u001b[39m(selected_obj.columns)\n\u001b[32m    467\u001b[39m )\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m selected_obj.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    470\u001b[39m     \u001b[38;5;66;03m# key only used for output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workspace/Env/ml/lib/python3.13/site-packages/pandas/core/apply.py:663\u001b[39m, in \u001b[36mApply.normalize_dictlike_arg\u001b[39m\u001b[34m(self, how, obj, func)\u001b[39m\n\u001b[32m    661\u001b[39m     cols = Index(\u001b[38;5;28mlist\u001b[39m(func.keys())).difference(obj.columns, sort=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    662\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumn(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m do not exist\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    665\u001b[39m aggregator_types = (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m    667\u001b[39m \u001b[38;5;66;03m# if we have a dict of any non-scalars\u001b[39;00m\n\u001b[32m    668\u001b[39m \u001b[38;5;66;03m# eg. {'A' : ['mean']}, normalize all to\u001b[39;00m\n\u001b[32m    669\u001b[39m \u001b[38;5;66;03m# be list-likes\u001b[39;00m\n\u001b[32m    670\u001b[39m \u001b[38;5;66;03m# Cannot use func.values() because arg may be a Series\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: \"Column(s) ['avg_neighbors'] do not exist\""
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# V2X SIMULATION ANALYSIS & ATTACK DETECTION NOTEBOOK\n",
    "# ============================================================================\n",
    "# Author: AI Assistant\n",
    "# Date: 2024\n",
    "# Description: Machine learning pipeline for V2X attack detection using\n",
    "#              simulation data from highway, mixed, and urban scenarios\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# # V2X Simulation Analysis & Attack Detection\n",
    "# \n",
    "# ## Overview\n",
    "# This notebook analyzes V2X simulation data to detect malicious vehicles using machine learning.\n",
    "# It processes data from multiple scenarios (highway, mixed, urban) and densities (50, 100, 150 vehicles/km).\n",
    "# \n",
    "# ## Key Features:\n",
    "# 1. **Network Statistics**: PDR, neighbor analysis, message statistics\n",
    "# 2. **Feature Engineering**: 5-second window features (message counts, RSSI stats, neighbor features)\n",
    "# 3. **ML Pipeline**: Random Forest with cross-validation and hyperparameter tuning\n",
    "# 4. **Trust Scoring**: Exponential smoothing model based on neighbor consistency\n",
    "# 5. **Visualization**: ROC curves, trust convergence plots, attack detection curves\n",
    "# 6. **Export**: LaTeX tables, model saving, comprehensive results\n",
    "# \n",
    "# ## Setup Instructions:\n",
    "# 1. Update `ROOT` variable below to point to your dataset\n",
    "# 2. Run all cells sequentially\n",
    "# 3. Check `/mnt/data/` for output files and figures\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. SETUP & CONFIGURATION\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import gc\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, roc_curve, confusion_matrix, classification_report,\n",
    "                             ConfusionMatrixDisplay)\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy import stats\n",
    "from scipy.signal import welch\n",
    "import scipy.spatial.distance as dist\n",
    "\n",
    "# Visualization\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # For headless environments\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Progress bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Configuration Variables\n",
    "\n",
    "# %%\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Set this to your dataset root directory\n",
    "ROOT = \"/home/jeanhuit/Documents/Workspace/simulation/results/\"\n",
    "# Alternative: Use symlink from /mnt/data/results to your actual data\n",
    "\n",
    "# Output directories\n",
    "FIG_DIR = Path(\"/home/jeanhuit/Documents/Workspace/simulation/figures\")\n",
    "OUTPUT_DIR = Path(\"/home/jeanhuit/Documents/Workspace/simulation/output\")\n",
    "MODEL_PATH = Path(\"/home/jeanhuit/Documents/Workspace/simulation/model\")\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Analysis parameters\n",
    "WINDOW_SIZE = 5  # seconds\n",
    "SLIDING_STEP = 1  # second for sliding window\n",
    "\n",
    "# ML parameters\n",
    "TEST_SIZE = 0.3\n",
    "CV_FOLDS = 5\n",
    "RF_N_ESTIMATORS = 100\n",
    "\n",
    "# Trust score parameters\n",
    "TRUST_ALPHA = 0.3  # Exponential smoothing factor\n",
    "CONSISTENCY_THRESHOLD = 0.7\n",
    "\n",
    "# Scenarios and densities to process\n",
    "SCENARIOS = [\"highway\", \"mixed\", \"urban\"]\n",
    "DENSITIES = [50, 100, 150]\n",
    "RUNS = [\"run-1\"]  # Can be extended to multiple runs\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  Root directory: {ROOT}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Figure directory: {FIG_DIR}\")\n",
    "print(f\"  Window size: {WINDOW_SIZE}s\")\n",
    "print(f\"  Scenarios: {SCENARIOS}\")\n",
    "print(f\"  Densities: {DENSITIES}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Utility Functions\n",
    "\n",
    "# %%\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def check_file_exists(filepath: str) -> bool:\n",
    "    \"\"\"Check if a file exists and print status.\"\"\"\n",
    "    exists = os.path.exists(filepath)\n",
    "    if not exists:\n",
    "        print(f\"  Warning: File not found - {filepath}\")\n",
    "    return exists\n",
    "\n",
    "def load_csv_safe(filepath: str, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"Load CSV file with error handling.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filepath):\n",
    "            df = pd.read_csv(filepath, **kwargs)\n",
    "            print(f\"  Loaded: {os.path.basename(filepath)} ({len(df)} rows)\")\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"  Warning: File not found - {filepath}\")\n",
    "            return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading {filepath}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def save_figure(fig, filename: str, dpi: int = 300):\n",
    "    \"\"\"Save figure to FIG_DIR.\"\"\"\n",
    "    filepath = os.path.join(FIG_DIR, filename)\n",
    "    fig.savefig(filepath, dpi=dpi, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"  Figure saved: {filepath}\")\n",
    "\n",
    "def save_dataframe(df: pd.DataFrame, filename: str):\n",
    "    \"\"\"Save dataframe to OUTPUT_DIR.\"\"\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"  Data saved: {filepath} ({len(df)} rows)\")\n",
    "\n",
    "def export_latex_table(df: pd.DataFrame, filename: str, caption: str = \"\", label: str = \"\"):\n",
    "    \"\"\"Export dataframe as LaTeX table.\"\"\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    latex_str = df.to_latex(index=False, caption=caption, label=label)\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write(latex_str)\n",
    "    print(f\"  LaTeX table saved: {filepath}\")\n",
    "\n",
    "def compute_pdr(sent: int, received: int) -> float:\n",
    "    \"\"\"Compute Packet Delivery Ratio.\"\"\"\n",
    "    if sent == 0:\n",
    "        return 0.0\n",
    "    return received / sent\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. DATA LOADING & NETWORK STATISTICS\n",
    "\n",
    "# %%\n",
    "# ============================================================================\n",
    "# DATA LOADER CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class V2XDataLoader:\n",
    "    \"\"\"Load and manage V2X simulation data from multiple scenarios.\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir: str):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.data = {}\n",
    "        self.stats = {}\n",
    "        \n",
    "    def load_scenario(self, scenario: str, density: int, run: str = \"run-1\"):\n",
    "        \"\"\"Load all log files for a specific scenario, density, and run.\"\"\"\n",
    "        scenario_path = self.root_dir / scenario / f\"density-{density}\" / run\n",
    "        \n",
    "        if not scenario_path.exists():\n",
    "            print(f\"Warning: Path not found - {scenario_path}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\nLoading {scenario} (density={density}, run={run})...\")\n",
    "        \n",
    "        # Load all CSV files\n",
    "        files = {\n",
    "            'bsm': load_csv_safe(scenario_path / \"bsm_log.csv\"),\n",
    "            'rssi': load_csv_safe(scenario_path / \"rssi_log.csv\"),\n",
    "            'neighbor': load_csv_safe(scenario_path / \"neighbor_log.csv\"),\n",
    "            'sybil': load_csv_safe(scenario_path / \"sybil_log.csv\"),\n",
    "            'replay': load_csv_safe(scenario_path / \"replay_log.csv\"),\n",
    "            'jammer': load_csv_safe(scenario_path / \"jammer_log.csv\")\n",
    "        }\n",
    "        \n",
    "        # Store data\n",
    "        key = f\"{scenario}_{density}_{run}\"\n",
    "        self.data[key] = files\n",
    "        \n",
    "        # Compute basic statistics\n",
    "        self._compute_basic_stats(key, files)\n",
    "        \n",
    "        return files\n",
    "    \n",
    "    def _compute_basic_stats(self, key: str, files: Dict):\n",
    "        \"\"\"Compute basic network statistics.\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        # BSM statistics\n",
    "        bsm_df = files.get('bsm', pd.DataFrame())\n",
    "        if not bsm_df.empty:\n",
    "            stats['bsm_sent'] = len(bsm_df)\n",
    "            stats['bsm_unique_senders'] = bsm_df['sender_id'].nunique() if 'sender_id' in bsm_df.columns else 0\n",
    "            \n",
    "            # If there's a receiver_id column\n",
    "            if 'receiver_id' in bsm_df.columns:\n",
    "                stats['bsm_received'] = bsm_df['receiver_id'].notna().sum()\n",
    "                stats['pdr'] = compute_pdr(stats['bsm_sent'], stats['bsm_received'])\n",
    "            else:\n",
    "                stats['bsm_received'] = 0\n",
    "                stats['pdr'] = 0\n",
    "        \n",
    "        # RSSI statistics\n",
    "        rssi_df = files.get('rssi', pd.DataFrame())\n",
    "        if not rssi_df.empty and 'rssi' in rssi_df.columns:\n",
    "            stats['rssi_mean'] = rssi_df['rssi'].mean()\n",
    "            stats['rssi_std'] = rssi_df['rssi'].std()\n",
    "            stats['rssi_min'] = rssi_df['rssi'].min()\n",
    "            stats['rssi_max'] = rssi_df['rssi'].max()\n",
    "        \n",
    "        # Neighbor statistics\n",
    "        neighbor_df = files.get('neighbor', pd.DataFrame())\n",
    "        if not neighbor_df.empty:\n",
    "            if 'neighbor_count' in neighbor_df.columns:\n",
    "                stats['avg_neighbors'] = neighbor_df['neighbor_count'].mean()\n",
    "                stats['max_neighbors'] = neighbor_df['neighbor_count'].max()\n",
    "            stats['unique_vehicles'] = self._count_unique_vehicles(neighbor_df)\n",
    "        \n",
    "        # Attack statistics\n",
    "        attack_stats = self._compute_attack_stats(files)\n",
    "        stats.update(attack_stats)\n",
    "        \n",
    "        self.stats[key] = stats\n",
    "        \n",
    "    def _count_unique_vehicles(self, neighbor_df: pd.DataFrame) -> int:\n",
    "        \"\"\"Count unique vehicles from neighbor logs.\"\"\"\n",
    "        vehicle_ids = set()\n",
    "        \n",
    "        # Check different possible column names\n",
    "        for col in ['vehicle_id', 'node_id', 'sender_id']:\n",
    "            if col in neighbor_df.columns:\n",
    "                vehicle_ids.update(neighbor_df[col].unique())\n",
    "        \n",
    "        # Also check neighbor lists if present\n",
    "        if 'neighbors' in neighbor_df.columns:\n",
    "            for neighbors in neighbor_df['neighbors'].dropna():\n",
    "                if isinstance(neighbors, str):\n",
    "                    try:\n",
    "                        neighbor_list = eval(neighbors) if '[' in neighbors else neighbors.split(',')\n",
    "                        vehicle_ids.update([str(n).strip() for n in neighbor_list])\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        return len(vehicle_ids)\n",
    "    \n",
    "    def _compute_attack_stats(self, files: Dict) -> Dict:\n",
    "        \"\"\"Compute attack-related statistics.\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        attack_dfs = {\n",
    "            'sybil': files.get('sybil'),\n",
    "            'replay': files.get('replay'),\n",
    "            'jammer': files.get('jammer')\n",
    "        }\n",
    "        \n",
    "        total_attack_rows = 0\n",
    "        unique_attackers = set()\n",
    "        \n",
    "        for attack_type, df in attack_dfs.items():\n",
    "            if df is not None and not df.empty:\n",
    "                rows = len(df)\n",
    "                stats[f'{attack_type}_events'] = rows\n",
    "                total_attack_rows += rows\n",
    "                \n",
    "                # Count unique attackers\n",
    "                for col in ['attacker_id', 'malicious_id', 'vehicle_id']:\n",
    "                    if col in df.columns:\n",
    "                        unique_attackers.update(df[col].unique())\n",
    "        \n",
    "        stats['total_attack_events'] = total_attack_rows\n",
    "        stats['unique_attackers'] = len(unique_attackers)\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def get_all_scenarios(self, scenarios: List[str], densities: List[int], runs: List[str]):\n",
    "        \"\"\"Load all specified scenarios, densities, and runs.\"\"\"\n",
    "        all_data = {}\n",
    "        \n",
    "        for scenario in scenarios:\n",
    "            for density in densities:\n",
    "                for run in runs:\n",
    "                    key = f\"{scenario}_{density}_{run}\"\n",
    "                    if key not in self.data:\n",
    "                        self.load_scenario(scenario, density, run)\n",
    "                    all_data[key] = self.data.get(key, {})\n",
    "        \n",
    "        return all_data\n",
    "    \n",
    "    def get_consolidated_stats(self) -> pd.DataFrame:\n",
    "        \"\"\"Create a consolidated DataFrame of all statistics.\"\"\"\n",
    "        rows = []\n",
    "        \n",
    "        for key, stats in self.stats.items():\n",
    "            # Parse key\n",
    "            parts = key.split('_')\n",
    "            if len(parts) >= 3:\n",
    "                scenario = parts[0]\n",
    "                density = int(parts[1])\n",
    "                run = '_'.join(parts[2:])\n",
    "                \n",
    "                row = {\n",
    "                    'scenario': scenario,\n",
    "                    'density': density,\n",
    "                    'run': run\n",
    "                }\n",
    "                row.update(stats)\n",
    "                rows.append(row)\n",
    "        \n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "# %%\n",
    "# Initialize data loader\n",
    "print(\"Initializing data loader...\")\n",
    "data_loader = V2XDataLoader(ROOT)\n",
    "\n",
    "# Load all data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING ALL SCENARIOS AND DENSITIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_data = data_loader.get_all_scenarios(SCENARIOS, DENSITIES, RUNS)\n",
    "\n",
    "# Display loaded data summary\n",
    "print(f\"\\nLoaded {len(all_data)} scenario-density-run combinations:\")\n",
    "for key in all_data.keys():\n",
    "    print(f\"  - {key}\")\n",
    "\n",
    "# %%\n",
    "# Compute and display consolidated statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NETWORK STATISTICS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "consolidated_stats = data_loader.get_consolidated_stats()\n",
    "\n",
    "if not consolidated_stats.empty:\n",
    "    # Display summary\n",
    "    print(\"\\nConsolidated Statistics:\")\n",
    "    print(consolidated_stats.to_string(index=False))\n",
    "    \n",
    "    # Save statistics\n",
    "    save_dataframe(consolidated_stats, \"consolidated_network_stats.csv\")\n",
    "    \n",
    "    # Export as LaTeX table\n",
    "    export_latex_table(\n",
    "        consolidated_stats,\n",
    "        \"network_stats_latex.tex\",\n",
    "        caption=\"Network Statistics for All Scenarios and Densities\",\n",
    "        label=\"tab:network_stats\"\n",
    "    )\n",
    "    \n",
    "    # Create summary by scenario\n",
    "    scenario_summary = consolidated_stats.groupby('scenario').agg({\n",
    "        'bsm_sent': 'mean',\n",
    "        'bsm_received': 'mean',\n",
    "        'pdr': 'mean',\n",
    "        'avg_neighbors': 'mean',\n",
    "        'total_attack_events': 'mean',\n",
    "        'unique_attackers': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nScenario-wise Summary:\")\n",
    "    print(scenario_summary)\n",
    "    \n",
    "    # Create summary by density\n",
    "    density_summary = consolidated_stats.groupby('density').agg({\n",
    "        'bsm_sent': 'mean',\n",
    "        'bsm_received': 'mean',\n",
    "        'pdr': 'mean',\n",
    "        'avg_neighbors': 'mean',\n",
    "        'total_attack_events': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nDensity-wise Summary:\")\n",
    "    print(density_summary)\n",
    "    \n",
    "else:\n",
    "    print(\"No statistics available. Check data loading.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. FEATURE ENGINEERING\n",
    "\n",
    "# %%\n",
    "# ============================================================================\n",
    "# FEATURE ENGINEERING CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"Extract features from V2X data in sliding time windows.\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size: int = 5, step: int = 1):\n",
    "        self.window_size = window_size\n",
    "        self.step = step\n",
    "        \n",
    "    def create_time_windows(self, df: pd.DataFrame, time_col: str = 'timestamp') -> List[Tuple]:\n",
    "        \"\"\"Create sliding time windows.\"\"\"\n",
    "        if df.empty:\n",
    "            return []\n",
    "        \n",
    "        if time_col not in df.columns:\n",
    "            print(f\"Warning: Time column '{time_col}' not found in dataframe\")\n",
    "            return []\n",
    "        \n",
    "        # Ensure timestamp is numeric\n",
    "        df = df.copy()\n",
    "        df[time_col] = pd.to_numeric(df[time_col], errors='coerce')\n",
    "        df = df.dropna(subset=[time_col])\n",
    "        \n",
    "        if df.empty:\n",
    "            return []\n",
    "        \n",
    "        min_time = df[time_col].min()\n",
    "        max_time = df[time_col].max()\n",
    "        \n",
    "        windows = []\n",
    "        start = min_time\n",
    "        while start <= max_time - self.window_size:\n",
    "            end = start + self.window_size\n",
    "            window_df = df[(df[time_col] >= start) & (df[time_col] < end)]\n",
    "            if not window_df.empty:\n",
    "                windows.append((start, end, window_df))\n",
    "            start += self.step\n",
    "        \n",
    "        return windows\n",
    "    \n",
    "    def extract_bsm_features(self, bsm_df: pd.DataFrame, vehicle_id: str = None) -> Dict:\n",
    "        \"\"\"Extract BSM-based features for a vehicle or overall.\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        if bsm_df.empty:\n",
    "            return self._get_empty_bsm_features()\n",
    "        \n",
    "        # Filter for specific vehicle if provided\n",
    "        if vehicle_id is not None and 'sender_id' in bsm_df.columns:\n",
    "            vehicle_bsm = bsm_df[bsm_df['sender_id'] == vehicle_id]\n",
    "        else:\n",
    "            vehicle_bsm = bsm_df\n",
    "        \n",
    "        # Message count features\n",
    "        features['msg_count'] = len(vehicle_bsm)\n",
    "        \n",
    "        # Unique sender/receiver features\n",
    "        if 'sender_id' in vehicle_bsm.columns:\n",
    "            features['unique_senders'] = vehicle_bsm['sender_id'].nunique()\n",
    "        \n",
    "        if 'receiver_id' in vehicle_bsm.columns:\n",
    "            features['unique_receivers'] = vehicle_bsm['receiver_id'].nunique()\n",
    "        \n",
    "        # Speed features (if available)\n",
    "        speed_cols = [col for col in vehicle_bsm.columns if 'speed' in col.lower()]\n",
    "        if speed_cols:\n",
    "            speed_data = vehicle_bsm[speed_cols[0]]\n",
    "            features.update({\n",
    "                'speed_mean': speed_data.mean(),\n",
    "                'speed_std': speed_data.std(),\n",
    "                'speed_min': speed_data.min(),\n",
    "                'speed_max': speed_data.max(),\n",
    "                'speed_range': speed_data.max() - speed_data.min() if len(speed_data) > 1 else 0\n",
    "            })\n",
    "        \n",
    "        # Temporal features\n",
    "        if 'timestamp' in vehicle_bsm.columns:\n",
    "            timestamps = vehicle_bsm['timestamp']\n",
    "            if len(timestamps) > 1:\n",
    "                intervals = np.diff(sorted(timestamps))\n",
    "                features.update({\n",
    "                    'msg_interval_mean': intervals.mean(),\n",
    "                    'msg_interval_std': intervals.std(),\n",
    "                    'msg_interval_min': intervals.min(),\n",
    "                    'msg_interval_max': intervals.max()\n",
    "                })\n",
    "                \n",
    "                # Spectral features (FFT of message intervals)\n",
    "                if len(intervals) >= 8:  # Minimum for meaningful FFT\n",
    "                    try:\n",
    "                        fft_values = np.abs(np.fft.fft(intervals - intervals.mean()))\n",
    "                        features['spectral_energy'] = np.sum(fft_values ** 2)\n",
    "                        features['spectral_entropy'] = self._compute_spectral_entropy(fft_values)\n",
    "                    except:\n",
    "                        features['spectral_energy'] = 0\n",
    "                        features['spectral_entropy'] = 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_rssi_features(self, rssi_df: pd.DataFrame, vehicle_id: str = None) -> Dict:\n",
    "        \"\"\"Extract RSSI-based features.\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        if rssi_df.empty:\n",
    "            return self._get_empty_rssi_features()\n",
    "        \n",
    "        # Filter for specific vehicle if provided\n",
    "        if vehicle_id is not None:\n",
    "            id_cols = [col for col in rssi_df.columns if 'id' in col or 'vehicle' in col]\n",
    "            if id_cols:\n",
    "                vehicle_rssi = rssi_df[rssi_df[id_cols[0]] == vehicle_id]\n",
    "            else:\n",
    "                vehicle_rssi = rssi_df\n",
    "        else:\n",
    "            vehicle_rssi = rssi_df\n",
    "        \n",
    "        if 'rssi' not in vehicle_rssi.columns or vehicle_rssi.empty:\n",
    "            return self._get_empty_rssi_features()\n",
    "        \n",
    "        rssi_values = vehicle_rssi['rssi'].dropna()\n",
    "        \n",
    "        if len(rssi_values) == 0:\n",
    "            return self._get_empty_rssi_features()\n",
    "        \n",
    "        # Basic statistics\n",
    "        features.update({\n",
    "            'rssi_mean': rssi_values.mean(),\n",
    "            'rssi_std': rssi_values.std(),\n",
    "            'rssi_min': rssi_values.min(),\n",
    "            'rssi_max': rssi_values.max(),\n",
    "            'rssi_range': rssi_values.max() - rssi_values.min(),\n",
    "            'rssi_variance': rssi_values.var()\n",
    "        })\n",
    "        \n",
    "        # Advanced statistics\n",
    "        features.update({\n",
    "            'rssi_skewness': stats.skew(rssi_values) if len(rssi_values) > 2 else 0,\n",
    "            'rssi_kurtosis': stats.kurtosis(rssi_values) if len(rssi_values) > 3 else 0,\n",
    "            'rssi_q1': np.percentile(rssi_values, 25),\n",
    "            'rssi_q3': np.percentile(rssi_values, 75),\n",
    "            'rss_iqr': np.percentile(rssi_values, 75) - np.percentile(rssi_values, 25)\n",
    "        })\n",
    "        \n",
    "        # Trend analysis (if enough data points)\n",
    "        if len(rssi_values) >= 3 and 'timestamp' in vehicle_rssi.columns:\n",
    "            time_sorted = vehicle_rssi.sort_values('timestamp')\n",
    "            rssi_sorted = time_sorted['rssi'].values\n",
    "            \n",
    "            # Linear trend\n",
    "            try:\n",
    "                x = np.arange(len(rssi_sorted))\n",
    "                slope, _, _, _, _ = stats.linregress(x, rssi_sorted)\n",
    "                features['rssi_trend_slope'] = slope\n",
    "                \n",
    "                # Moving statistics\n",
    "                if len(rssi_sorted) >= 5:\n",
    "                    window = min(5, len(rssi_sorted))\n",
    "                    moving_avg = np.convolve(rssi_sorted, np.ones(window)/window, mode='valid')\n",
    "                    features['rssi_moving_avg_change'] = moving_avg[-1] - moving_avg[0] if len(moving_avg) > 1 else 0\n",
    "            except:\n",
    "                features['rssi_trend_slope'] = 0\n",
    "                features['rssi_moving_avg_change'] = 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_neighbor_features(self, neighbor_df: pd.DataFrame, vehicle_id: str = None) -> Dict:\n",
    "        \"\"\"Extract neighbor-based features.\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        if neighbor_df.empty:\n",
    "            return self._get_empty_neighbor_features()\n",
    "        \n",
    "        # Filter for specific vehicle if provided\n",
    "        if vehicle_id is not None:\n",
    "            id_cols = [col for col in neighbor_df.columns if 'id' in col or 'vehicle' in col]\n",
    "            if id_cols:\n",
    "                vehicle_neighbors = neighbor_df[neighbor_df[id_cols[0]] == vehicle_id]\n",
    "            else:\n",
    "                vehicle_neighbors = neighbor_df\n",
    "        else:\n",
    "            vehicle_neighbors = neighbor_df\n",
    "        \n",
    "        if vehicle_neighbors.empty:\n",
    "            return self._get_empty_neighbor_features()\n",
    "        \n",
    "        # Count features\n",
    "        if 'neighbor_count' in vehicle_neighbors.columns:\n",
    "            counts = vehicle_neighbors['neighbor_count'].dropna()\n",
    "            if len(counts) > 0:\n",
    "                features.update({\n",
    "                    'neighbor_count_mean': counts.mean(),\n",
    "                    'neighbor_count_std': counts.std(),\n",
    "                    'neighbor_count_min': counts.min(),\n",
    "                    'neighbor_count_max': counts.max(),\n",
    "                    'neighbor_count_change': counts.iloc[-1] - counts.iloc[0] if len(counts) > 1 else 0\n",
    "                })\n",
    "        \n",
    "        # Neighbor consistency (if neighbor lists available)\n",
    "        if 'neighbors' in vehicle_neighbors.columns:\n",
    "            neighbor_lists = []\n",
    "            for neighbors in vehicle_neighbors['neighbors'].dropna():\n",
    "                if isinstance(neighbors, str):\n",
    "                    try:\n",
    "                        if '[' in neighbors:\n",
    "                            neighbor_list = eval(neighbors)\n",
    "                        else:\n",
    "                            neighbor_list = neighbors.split(',')\n",
    "                        neighbor_lists.append(set(str(n).strip() for n in neighbor_list))\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            if len(neighbor_lists) >= 2:\n",
    "                # Jaccard similarity between consecutive neighbor lists\n",
    "                jaccard_scores = []\n",
    "                for i in range(len(neighbor_lists) - 1):\n",
    "                    if neighbor_lists[i] and neighbor_lists[i+1]:\n",
    "                        intersection = neighbor_lists[i].intersection(neighbor_lists[i+1])\n",
    "                        union = neighbor_lists[i].union(neighbor_lists[i+1])\n",
    "                        jaccard = len(intersection) / len(union) if union else 0\n",
    "                        jaccard_scores.append(jaccard)\n",
    "                \n",
    "                if jaccard_scores:\n",
    "                    features.update({\n",
    "                        'neighbor_jaccard_mean': np.mean(jaccard_scores),\n",
    "                        'neighbor_jaccard_std': np.std(jaccard_scores),\n",
    "                        'neighbor_jaccard_min': np.min(jaccard_scores),\n",
    "                        'neighbor_jaccard_max': np.max(jaccard_scores)\n",
    "                    })\n",
    "        \n",
    "        # Neighbor duration (if timestamps available)\n",
    "        if 'timestamp' in vehicle_neighbors.columns and 'neighbor_count' in vehicle_neighbors.columns:\n",
    "            timestamps = vehicle_neighbors['timestamp'].values\n",
    "            counts = vehicle_neighbors['neighbor_count'].values\n",
    "            \n",
    "            if len(timestamps) > 1:\n",
    "                # Rate of neighbor change\n",
    "                time_diff = np.diff(timestamps)\n",
    "                count_diff = np.diff(counts)\n",
    "                valid_mask = time_diff > 0\n",
    "                \n",
    "                if np.any(valid_mask):\n",
    "                    change_rate = count_diff[valid_mask] / time_diff[valid_mask]\n",
    "                    features['neighbor_change_rate_mean'] = np.mean(np.abs(change_rate))\n",
    "                    features['neighbor_change_rate_std'] = np.std(change_rate)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_combined_features(self, data_dict: Dict, window_start: float, \n",
    "                                 window_end: float, vehicle_id: str = None) -> Dict:\n",
    "        \"\"\"Extract all features for a time window.\"\"\"\n",
    "        features = {\n",
    "            'window_start': window_start,\n",
    "            'window_end': window_end,\n",
    "            'vehicle_id': vehicle_id if vehicle_id else 'global'\n",
    "        }\n",
    "        \n",
    "        # Filter data for this window\n",
    "        window_data = {}\n",
    "        for data_type, df in data_dict.items():\n",
    "            if df is not None and not df.empty and 'timestamp' in df.columns:\n",
    "                window_df = df[(df['timestamp'] >= window_start) & (df['timestamp'] < window_end)]\n",
    "                window_data[data_type] = window_df\n",
    "            else:\n",
    "                window_data[data_type] = pd.DataFrame()\n",
    "        \n",
    "        # Extract BSM features\n",
    "        bsm_features = self.extract_bsm_features(window_data.get('bsm', pd.DataFrame()), vehicle_id)\n",
    "        features.update({f'bsm_{k}': v for k, v in bsm_features.items()})\n",
    "        \n",
    "        # Extract RSSI features\n",
    "        rssi_features = self.extract_rssi_features(window_data.get('rssi', pd.DataFrame()), vehicle_id)\n",
    "        features.update({f'rssi_{k}': v for k, v in rssi_features.items()})\n",
    "        \n",
    "        # Extract neighbor features\n",
    "        neighbor_features = self.extract_neighbor_features(window_data.get('neighbor', pd.DataFrame()), vehicle_id)\n",
    "        features.update({f'neighbor_{k}': v for k, v in neighbor_features.items()})\n",
    "        \n",
    "        # Combined features\n",
    "        if vehicle_id:\n",
    "            # Vehicle-specific consistency score\n",
    "            rssi_consistency = 1.0 / (1.0 + features.get('rssi_std', 1.0))\n",
    "            neighbor_consistency = features.get('neighbor_jaccard_mean', 0.5)\n",
    "            features['consistency_score'] = (rssi_consistency + neighbor_consistency) / 2\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_features_all_windows(self, data_dict: Dict, vehicle_ids: List[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Extract features for all vehicles across all time windows.\"\"\"\n",
    "        all_features = []\n",
    "        \n",
    "        # Get global time range from BSM data\n",
    "        bsm_df = data_dict.get('bsm', pd.DataFrame())\n",
    "        if bsm_df.empty:\n",
    "            print(\"Warning: No BSM data available for feature extraction\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Create time windows\n",
    "        windows = self.create_time_windows(bsm_df)\n",
    "        print(f\"Created {len(windows)} time windows of {self.window_size}s each\")\n",
    "        \n",
    "        # Extract features for each window\n",
    "        with tqdm(total=len(windows), desc=\"Extracting window features\") as pbar:\n",
    "            for window_start, window_end, window_df in windows:\n",
    "                # Global features (aggregated across all vehicles)\n",
    "                global_features = self.extract_combined_features(\n",
    "                    data_dict, window_start, window_end, None\n",
    "                )\n",
    "                all_features.append(global_features)\n",
    "                \n",
    "                # Per-vehicle features\n",
    "                if vehicle_ids:\n",
    "                    for vehicle_id in vehicle_ids:\n",
    "                        vehicle_features = self.extract_combined_features(\n",
    "                            data_dict, window_start, window_end, vehicle_id\n",
    "                        )\n",
    "                        all_features.append(vehicle_features)\n",
    "                \n",
    "                pbar.update(1)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        features_df = pd.DataFrame(all_features)\n",
    "        \n",
    "        # Fill NaN values\n",
    "        features_df = features_df.fillna(0)\n",
    "        \n",
    "        return features_df\n",
    "    \n",
    "    def _get_empty_bsm_features(self) -> Dict:\n",
    "        \"\"\"Return empty BSM features dictionary.\"\"\"\n",
    "        return {\n",
    "            'msg_count': 0,\n",
    "            'unique_senders': 0,\n",
    "            'unique_receivers': 0,\n",
    "            'speed_mean': 0,\n",
    "            'speed_std': 0,\n",
    "            'speed_min': 0,\n",
    "            'speed_max': 0,\n",
    "            'speed_range': 0,\n",
    "            'msg_interval_mean': 0,\n",
    "            'msg_interval_std': 0,\n",
    "            'msg_interval_min': 0,\n",
    "            'msg_interval_max': 0,\n",
    "            'spectral_energy': 0,\n",
    "            'spectral_entropy': 0\n",
    "        }\n",
    "    \n",
    "    def _get_empty_rssi_features(self) -> Dict:\n",
    "        \"\"\"Return empty RSSI features dictionary.\"\"\"\n",
    "        return {\n",
    "            'rssi_mean': 0,\n",
    "            'rssi_std': 0,\n",
    "            'rssi_min': 0,\n",
    "            'rssi_max': 0,\n",
    "            'rssi_range': 0,\n",
    "            'rssi_variance': 0,\n",
    "            'rssi_skewness': 0,\n",
    "            'rssi_kurtosis': 0,\n",
    "            'rssi_q1': 0,\n",
    "            'rssi_q3': 0,\n",
    "            'rss_iqr': 0,\n",
    "            'rssi_trend_slope': 0,\n",
    "            'rssi_moving_avg_change': 0\n",
    "        }\n",
    "    \n",
    "    def _get_empty_neighbor_features(self) -> Dict:\n",
    "        \"\"\"Return empty neighbor features dictionary.\"\"\"\n",
    "        return {\n",
    "            'neighbor_count_mean': 0,\n",
    "            'neighbor_count_std': 0,\n",
    "            'neighbor_count_min': 0,\n",
    "            'neighbor_count_max': 0,\n",
    "            'neighbor_count_change': 0,\n",
    "            'neighbor_jaccard_mean': 0,\n",
    "            'neighbor_jaccard_std': 0,\n",
    "            'neighbor_jaccard_min': 0,\n",
    "            'neighbor_jaccard_max': 0,\n",
    "            'neighbor_change_rate_mean': 0,\n",
    "            'neighbor_change_rate_std': 0\n",
    "        }\n",
    "    \n",
    "    def _compute_spectral_entropy(self, fft_values: np.ndarray) -> float:\n",
    "        \"\"\"Compute spectral entropy from FFT values.\"\"\"\n",
    "        if len(fft_values) == 0 or np.sum(fft_values) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Normalize to probability distribution\n",
    "        prob = fft_values / np.sum(fft_values)\n",
    "        \n",
    "        # Remove zeros for log calculation\n",
    "        prob = prob[prob > 0]\n",
    "        \n",
    "        # Compute entropy\n",
    "        entropy = -np.sum(prob * np.log2(prob))\n",
    "        \n",
    "        # Normalize by maximum entropy\n",
    "        max_entropy = np.log2(len(prob))\n",
    "        \n",
    "        return entropy / max_entropy if max_entropy > 0 else 0\n",
    "\n",
    "# %%\n",
    "# Initialize feature engineer\n",
    "print(\"\\nInitializing feature engineer...\")\n",
    "feature_engineer = FeatureEngineer(window_size=WINDOW_SIZE, step=SLIDING_STEP)\n",
    "\n",
    "# Extract features for each scenario\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE EXTRACTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_features = {}\n",
    "vehicle_id_cache = {}\n",
    "\n",
    "for key, data in all_data.items():\n",
    "    print(f\"\\nProcessing {key}...\")\n",
    "    \n",
    "    # Get vehicle IDs from BSM data\n",
    "    bsm_df = data.get('bsm', pd.DataFrame())\n",
    "    if not bsm_df.empty and 'sender_id' in bsm_df.columns:\n",
    "        vehicle_ids = bsm_df['sender_id'].unique()[:20]  # Limit to first 20 for performance\n",
    "        vehicle_id_cache[key] = vehicle_ids\n",
    "    else:\n",
    "        vehicle_ids = None\n",
    "    \n",
    "    # Extract features\n",
    "    features_df = feature_engineer.extract_features_all_windows(data, vehicle_ids)\n",
    "    \n",
    "    if not features_df.empty:\n",
    "        all_features[key] = features_df\n",
    "        print(f\"  Extracted {len(features_df)} feature rows\")\n",
    "        \n",
    "        # Save features for this scenario\n",
    "        save_dataframe(features_df, f\"features_{key}.csv\")\n",
    "    else:\n",
    "        print(f\"  No features extracted for {key}\")\n",
    "\n",
    "# Combine all features\n",
    "if all_features:\n",
    "    combined_features = pd.concat(all_features.values(), ignore_index=True)\n",
    "    print(f\"\\nTotal feature rows: {len(combined_features)}\")\n",
    "    print(f\"Total feature columns: {len(combined_features.columns)}\")\n",
    "    \n",
    "    # Display feature summary\n",
    "    print(\"\\nFeature columns:\")\n",
    "    for i, col in enumerate(combined_features.columns):\n",
    "        print(f\"  {i+1:3d}. {col}\")\n",
    "    \n",
    "    # Save combined features\n",
    "    save_dataframe(combined_features, \"all_features_combined.csv\")\n",
    "    \n",
    "else:\n",
    "    print(\"Warning: No features were extracted\")\n",
    "    combined_features = pd.DataFrame()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. LABEL GENERATION\n",
    "\n",
    "# %%\n",
    "# ============================================================================\n",
    "# LABEL GENERATOR CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class LabelGenerator:\n",
    "    \"\"\"Generate labels for ML training from attack logs.\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size: int = 5):\n",
    "        self.window_size = window_size\n",
    "    \n",
    "    def generate_labels(self, data_dict: Dict, windows: List[Tuple]) -> pd.DataFrame:\n",
    "        \"\"\"Generate labels for time windows based on attack logs.\"\"\"\n",
    "        labels = []\n",
    "        \n",
    "        # Get attack dataframes\n",
    "        attack_dfs = {\n",
    "            'sybil': data_dict.get('sybil', pd.DataFrame()),\n",
    "            'replay': data_dict.get('replay', pd.DataFrame()),\n",
    "            'jammer': data_dict.get('jammer', pd.DataFrame())\n",
    "        }\n",
    "        \n",
    "        # Process each window\n",
    "        for window_start, window_end, window_df in windows:\n",
    "            label_row = {\n",
    "                'window_start': window_start,\n",
    "                'window_end': window_end\n",
    "            }\n",
    "            \n",
    "            # Check each attack type\n",
    "            attacks_present = []\n",
    "            \n",
    "            for attack_type, attack_df in attack_dfs.items():\n",
    "                if attack_df.empty:\n",
    "                    continue\n",
    "                \n",
    "                # Check if attack occurred in this window\n",
    "                if 'timestamp' in attack_df.columns:\n",
    "                    window_attacks = attack_df[\n",
    "                        (attack_df['timestamp'] >= window_start) & \n",
    "                        (attack_df['timestamp'] < window_end)\n",
    "                    ]\n",
    "                    \n",
    "                    if not window_attacks.empty:\n",
    "                        attacks_present.append(attack_type)\n",
    "                        \n",
    "                        # Get attacker IDs if available\n",
    "                        for id_col in ['attacker_id', 'malicious_id', 'vehicle_id']:\n",
    "                            if id_col in window_attacks.columns:\n",
    "                                attackers = window_attacks[id_col].unique()\n",
    "                                label_row[f'{attack_type}_attackers'] = ','.join(map(str, attackers))\n",
    "                                break\n",
    "            \n",
    "            # Create binary label (malicious vs benign)\n",
    "            label_row['is_malicious'] = 1 if attacks_present else 0\n",
    "            \n",
    "            # Create multi-class label\n",
    "            if attacks_present:\n",
    "                if len(attacks_present) == 1:\n",
    "                    label_row['attack_type'] = attacks_present[0]\n",
    "                else:\n",
    "                    label_row['attack_type'] = 'mixed'\n",
    "            else:\n",
    "                label_row['attack_type'] = 'benign'\n",
    "            \n",
    "            # Count total attacks in window\n",
    "            label_row['attack_count'] = len(attacks_present)\n",
    "            \n",
    "            labels.append(label_row)\n",
    "        \n",
    "        return pd.DataFrame(labels)\n",
    "    \n",
    "    def generate_vehicle_labels(self, data_dict: Dict, windows: List[Tuple], \n",
    "                               vehicle_ids: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Generate per-vehicle labels for time windows.\"\"\"\n",
    "        all_labels = []\n",
    "        \n",
    "        # Get attack dataframes\n",
    "        attack_dfs = {\n",
    "            'sybil': data_dict.get('sybil', pd.DataFrame()),\n",
    "            'replay': data_dict.get('replay', pd.DataFrame()),\n",
    "            'jammer': data_dict.get('jammer', pd.DataFrame())\n",
    "        }\n",
    "        \n",
    "        # Process each vehicle in each window\n",
    "        for vehicle_id in tqdm(vehicle_ids, desc=\"Generating vehicle labels\"):\n",
    "            for window_start, window_end, window_df in windows:\n",
    "                label_row = {\n",
    "                    'window_start': window_start,\n",
    "                    'window_end': window_end,\n",
    "                    'vehicle_id': vehicle_id\n",
    "                }\n",
    "                \n",
    "                # Check if this vehicle was attacking in this window\n",
    "                is_attacker = False\n",
    "                attack_types = []\n",
    "                \n",
    "                for attack_type, attack_df in attack_dfs.items():\n",
    "                    if attack_df.empty:\n",
    "                        continue\n",
    "                    \n",
    "                    # Check if this vehicle appears as attacker\n",
    "                    for id_col in ['attacker_id', 'malicious_id', 'vehicle_id']:\n",
    "                        if id_col in attack_df.columns:\n",
    "                            # Filter for this vehicle and time window\n",
    "                            vehicle_attacks = attack_df[\n",
    "                                (attack_df[id_col] == vehicle_id) &\n",
    "                                (attack_df['timestamp'] >= window_start) & \n",
    "                                (attack_df['timestamp'] < window_end)\n",
    "                            ]\n",
    "                            \n",
    "                            if not vehicle_attacks.empty:\n",
    "                                is_attacker = True\n",
    "                                attack_types.append(attack_type)\n",
    "                                break\n",
    "                \n",
    "                # Create labels\n",
    "                label_row['is_malicious'] = 1 if is_attacker else 0\n",
    "                label_row['attack_type'] = ','.join(attack_types) if attack_types else 'benign'\n",
    "                label_row['attack_count'] = len(attack_types)\n",
    "                \n",
    "                all_labels.append(label_row)\n",
    "        \n",
    "        return pd.DataFrame(all_labels)\n",
    "    \n",
    "    def merge_features_labels(self, features_df: pd.DataFrame, labels_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Merge features and labels based on window times and vehicle IDs.\"\"\"\n",
    "        if features_df.empty or labels_df.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Determine merge columns\n",
    "        merge_cols = ['window_start', 'window_end']\n",
    "        if 'vehicle_id' in features_df.columns and 'vehicle_id' in labels_df.columns:\n",
    "            merge_cols.append('vehicle_id')\n",
    "        \n",
    "        # Merge\n",
    "        merged_df = pd.merge(\n",
    "            features_df, \n",
    "            labels_df, \n",
    "            on=merge_cols,\n",
    "            how='left'  # Keep all features, fill missing labels\n",
    "        )\n",
    "        \n",
    "        # Fill missing labels (assume benign)\n",
    "        if 'is_malicious' in merged_df.columns:\n",
    "            merged_df['is_malicious'] = merged_df['is_malicious'].fillna(0)\n",
    "        \n",
    "        if 'attack_type' in merged_df.columns:\n",
    "            merged_df['attack_type'] = merged_df['attack_type'].fillna('benign')\n",
    "        \n",
    "        if 'attack_count' in merged_df.columns:\n",
    "            merged_df['attack_count'] = merged_df['attack_count'].fillna(0)\n",
    "        \n",
    "        return merged_df\n",
    "\n",
    "# %%\n",
    "# Generate labels\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LABEL GENERATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "label_generator = LabelGenerator(window_size=WINDOW_SIZE)\n",
    "all_labels = {}\n",
    "\n",
    "for key, data in all_data.items():\n",
    "    print(f\"\\nGenerating labels for {key}...\")\n",
    "    \n",
    "    # Get BSM data for time windows\n",
    "    bsm_df = data.get('bsm', pd.DataFrame())\n",
    "    if bsm_df.empty:\n",
    "        print(f\"  No BSM data for {key}\")\n",
    "        continue\n",
    "    \n",
    "    # Create time windows\n",
    "    windows = feature_engineer.create_time_windows(bsm_df)\n",
    "    if not windows:\n",
    "        print(f\"  No time windows for {key}\")\n",
    "        continue\n",
    "    \n",
    "    # Generate global labels\n",
    "    global_labels = label_generator.generate_labels(data, windows)\n",
    "    \n",
    "    # Generate vehicle labels if vehicle IDs available\n",
    "    vehicle_ids = vehicle_id_cache.get(key)\n",
    "    if vehicle_ids is not None:\n",
    "        vehicle_labels = label_generator.generate_vehicle_labels(data, windows, vehicle_ids)\n",
    "        \n",
    "        # Combine labels\n",
    "        labels_df = pd.concat([global_labels, vehicle_labels], ignore_index=True)\n",
    "    else:\n",
    "        labels_df = global_labels\n",
    "    \n",
    "    if not labels_df.empty:\n",
    "        all_labels[key] = labels_df\n",
    "        print(f\"  Generated {len(labels_df)} label rows\")\n",
    "        print(f\"  Malicious windows: {labels_df['is_malicious'].sum()} ({labels_df['is_malicious'].mean():.1%})\")\n",
    "        \n",
    "        # Save labels\n",
    "        save_dataframe(labels_df, f\"labels_{key}.csv\")\n",
    "    else:\n",
    "        print(f\"  No labels generated for {key}\")\n",
    "\n",
    "# Combine all labels\n",
    "if all_labels:\n",
    "    combined_labels = pd.concat(all_labels.values(), ignore_index=True)\n",
    "    print(f\"\\nTotal label rows: {len(combined_labels)}\")\n",
    "    \n",
    "    # Display label distribution\n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    print(f\"  Benign windows: {len(combined_labels[combined_labels['is_malicious'] == 0])}\")\n",
    "    print(f\"  Malicious windows: {len(combined_labels[combined_labels['is_malicious'] == 1])}\")\n",
    "    \n",
    "    if 'attack_type' in combined_labels.columns:\n",
    "        print(\"\\nAttack Type Distribution:\")\n",
    "        print(combined_labels['attack_type'].value_counts())\n",
    "    \n",
    "    # Save combined labels\n",
    "    save_dataframe(combined_labels, \"all_labels_combined.csv\")\n",
    "    \n",
    "else:\n",
    "    print(\"Warning: No labels were generated\")\n",
    "    combined_labels = pd.DataFrame()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. EXPLORATORY DATA ANALYSIS\n",
    "\n",
    "# %%\n",
    "# ============================================================================\n",
    "# EXPLORATORY DATA ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Merge features and labels for analysis\n",
    "if not combined_features.empty and not combined_labels.empty:\n",
    "    print(\"\\nMerging features and labels...\")\n",
    "    merged_data = label_generator.merge_features_labels(combined_features, combined_labels)\n",
    "    \n",
    "    print(f\"Merged dataset shape: {merged_data.shape}\")\n",
    "    print(f\"Columns: {len(merged_data.columns)}\")\n",
    "    \n",
    "    # Save merged data\n",
    "    save_dataframe(merged_data, \"merged_features_labels.csv\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"Total samples: {len(merged_data)}\")\n",
    "    \n",
    "    if 'is_malicious' in merged_data.columns:\n",
    "        malicious_count = merged_data['is_malicious'].sum()\n",
    "        malicious_percent = malicious_count / len(merged_data) * 100\n",
    "        print(f\"Malicious samples: {malicious_count} ({malicious_percent:.2f}%)\")\n",
    "    \n",
    "    # Feature statistics\n",
    "    print(\"\\nFeature Statistics (first 20 numeric features):\")\n",
    "    numeric_cols = merged_data.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols[:20]:\n",
    "        if col not in ['window_start', 'window_end', 'is_malicious', 'attack_count']:\n",
    "            print(f\"  {col:30s}: mean={merged_data[col].mean():8.4f}, \"\n",
    "                  f\"std={merged_data[col].std():8.4f}, \"\n",
    "                  f\"range=[{merged_data[col].min():8.4f}, {merged_data[col].max():8.4f}]\")\n",
    "    \n",
    "    # Correlation analysis\n",
    "    print(\"\\nComputing feature correlations with labels...\")\n",
    "    if 'is_malicious' in merged_data.columns:\n",
    "        # Select numeric features only\n",
    "        feature_cols = [col for col in numeric_cols \n",
    "                       if col not in ['window_start', 'window_end', 'is_malicious', 'attack_count']\n",
    "                       and not col.startswith('attack')]\n",
    "        \n",
    "        if feature_cols:\n",
    "            correlations = []\n",
    "            for col in feature_cols:\n",
    "                corr = merged_data[col].corr(merged_data['is_malicious'])\n",
    "                correlations.append((col, corr))\n",
    "            \n",
    "            # Sort by absolute correlation\n",
    "            correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "            \n",
    "            print(\"\\nTop 10 features correlated with malicious label:\")\n",
    "            for col, corr in correlations[:10]:\n",
    "                print(f\"  {col:30s}: {corr:8.4f}\")\n",
    "    \n",
    "    # Create correlation heatmap for top features\n",
    "    if 'is_malicious' in merged_data.columns and len(feature_cols) >= 5:\n",
    "        print(\"\\nCreating correlation heatmap...\")\n",
    "        top_features = [col for col, _ in correlations[:15]] + ['is_malicious']\n",
    "        corr_matrix = merged_data[top_features].corr()\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                   center=0, square=True, ax=ax)\n",
    "        ax.set_title('Feature Correlation Matrix (Top 15 Features)', fontsize=14)\n",
    "        save_figure(fig, \"feature_correlation_heatmap.png\")\n",
    "        \n",
    "        # Plot feature distributions by class\n",
    "        print(\"\\nCreating feature distribution plots...\")\n",
    "        if len(feature_cols) >= 4:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            for i, col in enumerate(feature_cols[:4]):\n",
    "                # Separate by class\n",
    "                benign_data = merged_data[merged_data['is_malicious'] == 0][col].dropna()\n",
    "                malicious_data = merged_data[merged_data['is_malicious'] == 1][col].dropna()\n",
    "                \n",
    "                # Plot histograms\n",
    "                axes[i].hist(benign_data, alpha=0.5, label='Benign', bins=30, density=True)\n",
    "                axes[i].hist(malicious_data, alpha=0.5, label='Malicious', bins=30, density=True)\n",
    "                axes[i].set_xlabel(col)\n",
    "                axes[i].set_ylabel('Density')\n",
    "                axes[i].set_title(f'Distribution of {col}')\n",
    "                axes[i].legend()\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            save_figure(fig, \"feature_distributions_by_class.png\")\n",
    "    \n",
    "else:\n",
    "    print(\"Warning: Cannot perform EDA - features or labels are empty\")\n",
    "    merged_data = pd.DataFrame()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. ML PIPELINE & TRAINING\n",
    "\n",
    "# %%\n",
    "# ============================================================================\n",
    "# MACHINE LEARNING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MACHINE LEARNING PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if merged_data.empty or 'is_malicious' not in merged_data.columns:\n",
    "    print(\"Error: No labeled data available for ML training\")\n",
    "    print(\"Skipping ML pipeline...\")\n",
    "else:\n",
    "    # Prepare data for ML\n",
    "    print(\"\\nPreparing data for ML...\")\n",
    "    \n",
    "    # Select features (exclude metadata and labels)\n",
    "    exclude_cols = ['window_start', 'window_end', 'vehicle_id', \n",
    "                   'is_malicious', 'attack_type', 'attack_count',\n",
    "                   'sybil_attackers', 'replay_attackers', 'jammer_attackers']\n",
    "    \n",
    "    feature_cols = [col for col in merged_data.columns \n",
    "                   if col not in exclude_cols \n",
    "                   and merged_data[col].dtype in [np.int64, np.float64]]\n",
    "    \n",
    "    print(f\"Selected {len(feature_cols)} features for ML\")\n",
    "    \n",
    "    # Create feature matrix and labels\n",
    "    X = merged_data[feature_cols].fillna(0).values\n",
    "    y = merged_data['is_malicious'].values\n",
    "    \n",
    "    print(f\"Feature matrix shape: {X.shape}\")\n",
    "    print(f\"Label distribution: {np.bincount(y.astype(int))}\")\n",
    "    \n",
    "    # Check class balance\n",
    "    class_counts = np.bincount(y.astype(int))\n",
    "    if len(class_counts) > 1:\n",
    "        print(f\"Class balance: {class_counts[0]} benign vs {class_counts[1]} malicious\")\n",
    "        print(f\"Imbalance ratio: {class_counts[0]/class_counts[1]:.2f}:1\")\n",
    "    \n",
    "    # Train-test split\n",
    "    print(\"\\nSplitting data into train/test sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # Create and train Random Forest model\n",
    "    print(\"\\n\" + \"-\"*40)\n",
    "    print(\"RANDOM FOREST CLASSIFIER\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Define pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', RandomForestClassifier(\n",
    "            n_estimators=RF_N_ESTIMATORS,\n",
    "            random_state=RANDOM_SEED,\n",
    "            n_jobs=-1,\n",
    "            class_weight='balanced'  # Handle class imbalance\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Cross-validation\n",
    "    print(\"\\nPerforming cross-validation...\")\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, \n",
    "                               cv=CV_FOLDS, scoring='f1_weighted', n_jobs=-1)\n",
    "    \n",
    "    print(f\"Cross-validation F1 scores: {cv_scores}\")\n",
    "    print(f\"Mean CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    \n",
    "    # Hyperparameter tuning\n",
    "    print(\"\\nPerforming hyperparameter tuning...\")\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__max_depth': [None, 10, 20, 30],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, param_grid, \n",
    "        cv=CV_FOLDS, \n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    print(\"\\nTraining final model...\")\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Save model\n",
    "    print(f\"\\nSaving model to {MODEL_PATH}...\")\n",
    "    with open(MODEL_PATH, 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "    \n",
    "    print(\"Model saved successfully!\")\n",
    "    \n",
    "    # Feature importance\n",
    "    print(\"\\nComputing feature importance...\")\n",
    "    if hasattr(best_model.named_steps['classifier'], 'feature_importances_'):\n",
    "        importances = best_model.named_steps['classifier'].feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        print(\"\\nTop 20 most important features:\")\n",
    "        for i in range(min(20, len(feature_cols))):\n",
    "            idx = indices[i]\n",
    "            print(f\"  {i+1:2d}. {feature_cols[idx]:30s}: {importances[idx]:.6f}\")\n",
    "        \n",
    "        # Plot feature importance\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        top_n = min(20, len(feature_cols))\n",
    "        ax.barh(range(top_n), importances[indices[:top_n]][::-1])\n",
    "        ax.set_yticks(range(top_n))\n",
    "        ax.set_yticklabels([feature_cols[i] for i in indices[:top_n]][::-1])\n",
    "        ax.set_xlabel('Feature Importance')\n",
    "        ax.set_title('Top 20 Feature Importances')\n",
    "        plt.tight_layout()\n",
    "        save_figure(fig, \"feature_importance.png\")\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"\\nMaking predictions on test set...\")\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, 'predict_proba') else None\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. MODEL EVALUATION\n",
    "\n",
    "# %%\n",
    "# ============================================================================\n",
    "# MODEL EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'y_test' in locals() and 'y_pred' in locals():\n",
    "    # Compute metrics\n",
    "    print(\"\\nComputing evaluation metrics...\")\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Benign', 'Malicious'], zero_division=0))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Benign', 'Malicious'])\n",
    "    disp.plot(cmap='Blues', ax=ax)\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    save_figure(fig, \"confusion_matrix.png\")\n",
    "    \n",
    "    # ROC Curve and AUC (if probability estimates available)\n",
    "    if y_pred_proba is not None:\n",
    "        print(\"\\nComputing ROC/AUC...\")\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        print(f\"AUC Score: {auc_score:.4f}\")\n",
    "        \n",
    "        # Plot ROC curve\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.plot(fpr, tpr, 'b-', label=f'ROC curve (AUC = {auc_score:.3f})')\n",
    "        ax.plot([0, 1], [0, 1], 'r--', label='Random classifier')\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        ax.legend(loc='lower right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        save_figure(fig, \"roc_curve.png\")\n",
    "    \n",
    "    # Create metrics dataframe\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC'],\n",
    "        'Value': [accuracy, precision, recall, f1, auc_score if 'auc_score' in locals() else np.nan]\n",
    "    })\n",
    "    \n",
    "    # Save metrics\n",
    "    save_dataframe(metrics_df, \"ml_metrics.csv\")\n",
    "    \n",
    "    # Export metrics as LaTeX table\n",
    "    export_latex_table(\n",
    "        metrics_df.round(4),\n",
    "        \"ml_metrics_latex.tex\",\n",
    "        caption=\"Machine Learning Performance Metrics\",\n",
    "        label=\"tab:ml_metrics\"\n",
    "    )\n",
    "    \n",
    "    # Per-class metrics\n",
    "    if len(np.unique(y_test)) > 1:\n",
    "        per_class_metrics = []\n",
    "        classes = ['Benign', 'Malicious']\n",
    "        \n",
    "        for i, class_name in enumerate(classes):\n",
    "            if i < len(np.unique(y_test)):\n",
    "                # Create binary labels for this class\n",
    "                y_test_binary = (y_test == i).astype(int)\n",
    "                y_pred_binary = (y_pred == i).astype(int)\n",
    "                \n",
    "                if len(np.unique(y_test_binary)) > 1:\n",
    "                    class_accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
    "                    class_precision = precision_score(y_test_binary, y_pred_binary, zero_division=0)\n",
    "                    class_recall = recall_score(y_test_binary, y_pred_binary, zero_division=0)\n",
    "                    class_f1 = f1_score(y_test_binary, y_pred_binary, zero_division=0)\n",
    "                    \n",
    "                    per_class_metrics.append({\n",
    "                        'Class': class_name,\n",
    "                        'Accuracy': class_accuracy,\n",
    "                        'Precision': class_precision,\n",
    "                        'Recall': class_recall,\n",
    "                        'F1-Score': class_f1\n",
    "                    })\n",
    "        \n",
    "        if per_class_metrics:\n",
    "            per_class_df = pd.DataFrame(per_class_metrics)\n",
    "            print(\"\\nPer-class Metrics:\")\n",
    "            print(per_class_df.to_string(index=False))\n",
    "            \n",
    "            save_dataframe(per_class_df, \"per_class_metrics.csv\")\n",
    "            \n",
    "            # Export as LaTeX\n",
    "            export_latex_table(\n",
    "                per_class_df.round(4),\n",
    "                \"per_class_metrics_latex.tex\",\n",
    "                caption=\"Per-class Performance Metrics\",\n",
    "                label=\"tab:per_class_metrics\"\n",
    "            )\n",
    "    \n",
    "    # Scenario-wise evaluation (if scenario information is available)\n",
    "    print(\"\\n\" + \"-\"*40)\n",
    "    print(\"SCENARIO-WISE EVALUATION\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Try to extract scenario from data\n",
    "    if 'scenario' in merged_data.columns:\n",
    "        # We need to map test indices back to original data\n",
    "        test_indices = merged_data.index[merged_data.index.isin(\n",
    "            merged_data.sample(frac=TEST_SIZE, random_state=RANDOM_SEED).index\n",
    "        )]\n",
    "        \n",
    "        scenario_results = []\n",
    "        for scenario in merged_data['scenario'].unique():\n",
    "            scenario_mask = merged_data['scenario'] == scenario\n",
    "            scenario_test_mask = scenario_mask & merged_data.index.isin(test_indices)\n",
    "            \n",
    "            if scenario_test_mask.any():\n",
    "                scenario_y_true = merged_data.loc[scenario_test_mask, 'is_malicious'].values\n",
    "                scenario_y_pred = y_pred[scenario_test_mask[scenario_test_mask].index]\n",
    "                \n",
    "                scenario_accuracy = accuracy_score(scenario_y_true, scenario_y_pred)\n",
    "                scenario_f1 = f1_score(scenario_y_true, scenario_y_pred, average='weighted', zero_division=0)\n",
    "                \n",
    "                scenario_results.append({\n",
    "                    'Scenario': scenario,\n",
    "                    'Samples': len(scenario_y_true),\n",
    "                    'Accuracy': scenario_accuracy,\n",
    "                    'F1-Score': scenario_f1\n",
    "                })\n",
    "        \n",
    "        if scenario_results:\n",
    "            scenario_df = pd.DataFrame(scenario_results)\n",
    "            print(\"\\nScenario-wise Performance:\")\n",
    "            print(scenario_df.to_string(index=False))\n",
    "            \n",
    "            save_dataframe(scenario_df, \"scenario_metrics.csv\")\n",
    "            \n",
    "            # Plot scenario comparison\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            x = range(len(scenario_df))\n",
    "            width = 0.35\n",
    "            \n",
    "            ax.bar([i - width/2 for i in x], scenario_df['Accuracy'], width, label='Accuracy')\n",
    "            ax.bar([i + width/2 for i in x], scenario_df['F1-Score'], width, label='F1-Score')\n",
    "            \n",
    "            ax.set_xlabel('Scenario')\n",
    "            ax.set_ylabel('Score')\n",
    "            ax.set_title('Model Performance by Scenario')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(scenario_df['Scenario'])\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            save_figure(fig, \"scenario_performance.png\")\n",
    "    \n",
    "else:\n",
    "    print(\"Warning: No model predictions available for evaluation\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. TRUST SCORE COMPUTATION\n",
    "\n",
    "# %%\n",
    "# ============================================================================\n",
    "# TRUST SCORE COMPUTATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRUST SCORE COMPUTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class TrustScoreCalculator:\n",
    "    \"\"\"Calculate trust scores using exponential smoothing model.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: float = 0.3, threshold: float = 0.7):\n",
    "        self.alpha = alpha  # Smoothing factor\n",
    "        self.threshold = threshold  # Consistency threshold\n",
    "        self.trust_scores = {}\n",
    "    \n",
    "    def compute_consistency_score(self, features: Dict) -> float:\n",
    "        \"\"\"Compute consistency score from features.\"\"\"\n",
    "        consistency_metrics = []\n",
    "        \n",
    "        # RSSI consistency (inverse of variance)\n",
    "        if 'rssi_std' in features:\n",
    "            rssi_consistency = 1.0 / (1.0 + features['rssi_std'])\n",
    "            consistency_metrics.append(rssi_consistency)\n",
    "        \n",
    "        # Neighbor consistency\n",
    "        if 'neighbor_jaccard_mean' in features:\n",
    "            neighbor_consistency = features['neighbor_jaccard_mean']\n",
    "            consistency_metrics.append(neighbor_consistency)\n",
    "        \n",
    "        # Message interval consistency (inverse of std)\n",
    "        if 'msg_interval_std' in features and features['msg_interval_std'] > 0:\n",
    "            msg_consistency = 1.0 / (1.0 + features['msg_interval_std'])\n",
    "            consistency_metrics.append(msg_consistency)\n",
    "        \n",
    "        # Speed consistency\n",
    "        if 'speed_std' in features and features['speed_std'] > 0:\n",
    "            speed_consistency = 1.0 / (1.0 + features['speed_std'])\n",
    "            consistency_metrics.append(speed_consistency)\n",
    "        \n",
    "        if consistency_metrics:\n",
    "            return np.mean(consistency_metrics)\n",
    "        else:\n",
    "            return 0.5  # Default neutral score\n",
    "    \n",
    "    def update_trust_score(self, vehicle_id: str, consistency: float, \n",
    "                          previous_trust: float = None) -> float:\n",
    "        \"\"\"Update trust score using exponential smoothing.\"\"\"\n",
    "        if previous_trust is None:\n",
    "            # Initialize trust score\n",
    "            trust = consistency\n",
    "        else:\n",
    "            # Exponential smoothing: trust_t =  * consistency_t + (1-) * trust_{t-1}\n",
    "            trust = self.alpha * consistency + (1 - self.alpha) * previous_trust\n",
    "        \n",
    "        # Store trust score\n",
    "        self.trust_scores[vehicle_id] = trust\n",
    "        \n",
    "        return trust\n",
    "    \n",
    "    def compute_trust_scores_over_time(self, features_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Compute trust scores for all vehicles over time.\"\"\"\n",
    "        if features_df.empty or 'vehicle_id' not in features_df.columns:\n",
    "            print(\"Warning: No vehicle-specific features for trust computation\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Sort by vehicle and time\n",
    "        features_sorted = features_df.sort_values(['vehicle_id', 'window_start'])\n",
    "        \n",
    "        trust_records = []\n",
    "        current_trust = {}\n",
    "        \n",
    "        # Group by vehicle\n",
    "        for vehicle_id, group in tqdm(features_sorted.groupby('vehicle_id'), \n",
    "                                     desc=\"Computing trust scores\"):\n",
    "            vehicle_trust = []\n",
    "            \n",
    "            for _, row in group.iterrows():\n",
    "                # Convert row to dictionary\n",
    "                row_dict = row.to_dict()\n",
    "                \n",
    "                # Compute consistency score\n",
    "                consistency = self.compute_consistency_score(row_dict)\n",
    "                \n",
    "                # Get previous trust score\n",
    "                prev_trust = current_trust.get(vehicle_id, 0.5)  # Start with neutral trust\n",
    "                \n",
    "                # Update trust score\n",
    "                trust = self.update_trust_score(vehicle_id, consistency, prev_trust)\n",
    "                current_trust[vehicle_id] = trust\n",
    "                \n",
    "                # Record trust\n",
    "                trust_record = {\n",
    "                    'vehicle_id': vehicle_id,\n",
    "                    'window_start': row['window_start'],\n",
    "                    'window_end': row['window_end'],\n",
    "                    'consistency_score': consistency,\n",
    "                    'trust_score': trust,\n",
    "                    'is_trusted': trust >= self.threshold\n",
    "                }\n",
    "                \n",
    "                # Add additional features if available\n",
    "                for col in ['rssi_std', 'neighbor_jaccard_mean', 'msg_interval_std', 'speed_std']:\n",
    "                    if col in row_dict:\n",
    "                        trust_record[col] = row_dict[col]\n",
    "                \n",
    "                trust_records.append(trust_record)\n",
    "                vehicle_trust.append(trust)\n",
    "            \n",
    "            # Store final trust score for this vehicle\n",
    "            if vehicle_trust:\n",
    "                self.trust_scores[vehicle_id] = vehicle_trust[-1]\n",
    "        \n",
    "        trust_df = pd.DataFrame(trust_records)\n",
    "        \n",
    "        return trust_df\n",
    "    \n",
    "    def plot_trust_convergence(self, trust_df: pd.DataFrame, n_vehicles: int = 10):\n",
    "        \"\"\"Plot trust score convergence over time for selected vehicles.\"\"\"\n",
    "        if trust_df.empty:\n",
    "            print(\"Warning: No trust data to plot\")\n",
    "            return\n",
    "        \n",
    "        # Select random vehicles to plot\n",
    "        vehicle_ids = trust_df['vehicle_id'].unique()\n",
    "        if len(vehicle_ids) > n_vehicles:\n",
    "            plot_vehicles = np.random.choice(vehicle_ids, n_vehicles, replace=False)\n",
    "        else:\n",
    "            plot_vehicles = vehicle_ids\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "        \n",
    "        # Plot 1: Individual trust convergence\n",
    "        ax1 = axes[0]\n",
    "        for vehicle_id in plot_vehicles:\n",
    "            vehicle_data = trust_df[trust_df['vehicle_id'] == vehicle_id].sort_values('window_start')\n",
    "            if not vehicle_data.empty:\n",
    "                ax1.plot(vehicle_data['window_start'], vehicle_data['trust_score'], \n",
    "                        marker='o', markersize=3, label=vehicle_id, alpha=0.7)\n",
    "        \n",
    "        ax1.axhline(y=self.threshold, color='r', linestyle='--', label=f'Threshold ({self.threshold})')\n",
    "        ax1.set_xlabel('Time (window start)')\n",
    "        ax1.set_ylabel('Trust Score')\n",
    "        ax1.set_title('Trust Score Convergence Over Time')\n",
    "        ax1.legend(loc='upper right', fontsize='small')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Distribution of final trust scores\n",
    "        ax2 = axes[1]\n",
    "        final_trust = trust_df.groupby('vehicle_id')['trust_score'].last()\n",
    "        \n",
    "        ax2.hist(final_trust, bins=30, alpha=0.7, edgecolor='black')\n",
    "        ax2.axvline(x=self.threshold, color='r', linestyle='--', label=f'Threshold ({self.threshold})')\n",
    "        ax2.set_xlabel('Final Trust Score')\n",
    "        ax2.set_ylabel('Number of Vehicles')\n",
    "        ax2.set_title('Distribution of Final Trust Scores')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_figure(fig, \"trust_convergence.png\")\n",
    "        \n",
    "        # Additional plot: Trust vs Consistency\n",
    "        fig2, ax = plt.subplots(figsize=(10, 6))\n",
    "        scatter = ax.scatter(trust_df['consistency_score'], trust_df['trust_score'], \n",
    "                           c=trust_df['trust_score'], cmap='viridis', alpha=0.6)\n",
    "        ax.set_xlabel('Consistency Score')\n",
    "        ax.set_ylabel('Trust Score')\n",
    "        ax.set_title('Trust Score vs Consistency Score')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(scatter, ax=ax, label='Trust Score')\n",
    "        \n",
    "        # Add diagonal line (trust = consistency for no memory)\n",
    "        x = np.linspace(0, 1, 100)\n",
    "        ax.plot(x, x, 'r--', alpha=0.5, label='Trust = Consistency (no memory)')\n",
    "        ax.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_figure(fig2, \"trust_vs_consistency.png\")\n",
    "\n",
    "# %%\n",
    "# Compute trust scores\n",
    "print(\"\\nComputing trust scores...\")\n",
    "\n",
    "trust_calculator = TrustScoreCalculator(alpha=TRUST_ALPHA, threshold=CONSISTENCY_THRESHOLD)\n",
    "\n",
    "# Use vehicle-specific features if available\n",
    "if 'vehicle_id' in combined_features.columns and combined_features['vehicle_id'].nunique() > 1:\n",
    "    trust_df = trust_calculator.compute_trust_scores_over_time(combined_features)\n",
    "    \n",
    "    if not trust_df.empty:\n",
    "        print(f\"Computed trust scores for {trust_df['vehicle_id'].nunique()} vehicles\")\n",
    "        print(f\"Average trust score: {trust_df['trust_score'].mean():.4f}\")\n",
    "        print(f\"Trusted vehicles: {trust_df['is_trusted'].sum()} / {trust_df['vehicle_id'].nunique()}\")\n",
    "        \n",
    "        # Save trust scores\n",
    "        save_dataframe(trust_df, \"trust_scores.csv\")\n",
    "        \n",
    "        # Plot trust convergence\n",
    "        print(\"\\nCreating trust convergence plots...\")\n",
    "        trust_calculator.plot_trust_convergence(trust_df, n_vehicles=15)\n",
    "        \n",
    "        # Merge trust scores with labels for analysis\n",
    "        if 'is_malicious' in merged_data.columns:\n",
    "            # We need to merge based on vehicle_id and window times\n",
    "            merged_trust = pd.merge(\n",
    "                trust_df,\n",
    "                merged_data[['vehicle_id', 'window_start', 'window_end', 'is_malicious', 'attack_type']],\n",
    "                on=['vehicle_id', 'window_start', 'window_end'],\n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            # Analyze trust vs malicious behavior\n",
    "            if 'is_malicious' in merged_trust.columns:\n",
    "                print(\"\\nTrust Analysis by Behavior:\")\n",
    "                benign_trust = merged_trust[merged_trust['is_malicious'] == 0]['trust_score']\n",
    "                malicious_trust = merged_trust[merged_trust['is_malicious'] == 1]['trust_score']\n",
    "                \n",
    "                print(f\"  Benign vehicles average trust: {benign_trust.mean():.4f}\")\n",
    "                print(f\"  Malicious vehicles average trust: {malicious_trust.mean():.4f}\")\n",
    "                \n",
    "                # Statistical test\n",
    "                if len(benign_trust) > 1 and len(malicious_trust) > 1:\n",
    "                    t_stat, p_value = stats.ttest_ind(benign_trust, malicious_trust, equal_var=False)\n",
    "                    print(f\"  T-test: t={t_stat:.4f}, p={p_value:.6f}\")\n",
    "                    \n",
    "                    # Plot trust distribution by behavior\n",
    "                    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                    \n",
    "                    ax.hist(benign_trust, bins=30, alpha=0.5, label='Benign', density=True)\n",
    "                    ax.hist(malicious_trust, bins=30, alpha=0.5, label='Malicious', density=True)\n",
    "                    \n",
    "                    ax.set_xlabel('Trust Score')\n",
    "                    ax.set_ylabel('Density')\n",
    "                    ax.set_title('Trust Score Distribution by Behavior')\n",
    "                    ax.legend()\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    save_figure(fig, \"trust_distribution_by_behavior.png\")\n",
    "    else:\n",
    "        print(\"Warning: No trust scores computed\")\n",
    "else:\n",
    "    print(\"Warning: Insufficient vehicle-specific data for trust computation\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. VISUALIZATION\n",
    "\n",
    "# %%\n",
    "# ============================================================================\n",
    "# COMPREHENSIVE VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VISUALIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def create_comprehensive_visualizations():\n",
    "    \"\"\"Create all visualizations and save to FIG_DIR.\"\"\"\n",
    "    \n",
    "    print(\"\\nCreating comprehensive visualizations...\")\n",
    "    \n",
    "    # 1. Network Statistics Overview\n",
    "    if not consolidated_stats.empty:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # PDR by scenario\n",
    "        ax = axes[0, 0]\n",
    "        scenario_pdr = consolidated_stats.groupby('scenario')['pdr'].mean()\n",
    "        scenario_pdr.plot(kind='bar', ax=ax, color='skyblue')\n",
    "        ax.set_title('Packet Delivery Ratio by Scenario')\n",
    "        ax.set_ylabel('PDR')\n",
    "        ax.set_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6628eacf-72cc-44eb-aebd-a5f313ec2fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
