{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VANET Attack Detection and Mitigation Analysis\n",
    "\n",
    "This notebook analyzes the output from the VANET simulation that includes DDoS, Sybil, Replay, Jamming, and Message Falsification attacks with various mitigation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all generated CSV files\n",
    "bsm_df = pd.read_csv('bsm_log.csv')\n",
    "attack_df = pd.read_csv('attack_log.csv')\n",
    "mitigation_df = pd.read_csv('mitigation_log.csv')\n",
    "trust_df = pd.read_csv('trust_log.csv')\n",
    "ml_detection_df = pd.read_csv('ml_detection_log.csv')\n",
    "neighbor_df = pd.read_csv('neighbor_log.csv')\n",
    "jammer_df = pd.read_csv('jammer_log.csv')\n",
    "sybil_df = pd.read_csv('sybil_log.csv')\n",
    "ddos_df = pd.read_csv('ddos_log.csv')\n",
    "msg_falsification_df = pd.read_csv('msg_falsification_log.csv')\n",
    "features_df = pd.read_csv('features_log.csv')\n",
    "detection_df = pd.read_csv('detection_log.csv')\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"BSM Log: {bsm_df.shape}\")\n",
    "print(f\"Attack Log: {attack_df.shape}\")\n",
    "print(f\"Mitigation Log: {mitigation_df.shape}\")\n",
    "print(f\"Trust Log: {trust_df.shape}\")\n",
    "print(f\"ML Detection Log: {ml_detection_df.shape}\")\n",
    "print(f\"Neighbor Log: {neighbor_df.shape}\")\n",
    "print(f\"Jammer Log: {jammer_df.shape}\")\n",
    "print(f\"Sybil Log: {sybil_df.shape}\")\n",
    "print(f\"DDoS Log: {ddos_df.shape}\")\n",
    "print(f\"Message Falsification Log: {msg_falsification_df.shape}\")\n",
    "print(f\"Features Log: {features_df.shape}\")\n",
    "print(f\"Detection Log: {detection_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic info about BSM data\n",
    "print(\"BSM Data Overview:\")\n",
    "print(bsm_df.head())\n",
    "print(f\"\\nData types:\\n{bsm_df.dtypes}\")\n",
    "print(f\"\\nBasic statistics:\\n{bsm_df.describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature data\n",
    "print(\"Features Data Overview:\")\n",
    "print(features_df.head())\n",
    "print(f\"\\nData types:\\n{features_df.dtypes}\")\n",
    "print(f\"\\nBasic statistics:\\n{features_df.describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualization of VANET Behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot vehicle positions over time\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(bsm_df['posX'], bsm_df['posY'], alpha=0.5, c=bsm_df['timestamp'], cmap='viridis')\n",
    "plt.colorbar(label='Timestamp')\n",
    "plt.xlabel('X Position')\n",
    "plt.ylabel('Y Position')\n",
    "plt.title('Vehicle Positions Over Time')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot velocity distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(bsm_df['velX'], bins=50, alpha=0.7, label='X Velocity')\n",
    "plt.xlabel('Velocity X')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('X Velocity Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(bsm_df['velY'], bins=50, alpha=0.7, label='Y Velocity')\n",
    "plt.xlabel('Velocity Y')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Y Velocity Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attack Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze DDoS attacks\n",
    "print(\"DDoS Attack Details:\")\n",
    "print(f\"Total DDoS attack records: {len(ddos_df)}\")\n",
    "print(f\"DDoS attackers: {ddos_df['attackerId'].unique()}\")\n",
    "print(f\"Attack details sample:\\n{ddos_df.head(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Sybil attacks\n",
    "print(\"Sybil Attack Details:\")\n",
    "print(f\"Total Sybil attack records: {len(sybil_df)}\")\n",
    "print(f\"Sybil attackers: {sybil_df['attackerId'].unique()}\")\n",
    "print(f\"Fake IDs count: {len(sybil_df['fakeId'].unique())}\")\n",
    "print(f\"Sybil attack sample:\\n{sybil_df.head(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Jamming attacks\n",
    "print(\"Jamming Attack Details:\")\n",
    "print(f\"Total Jamming records: {len(jammer_df)}\")\n",
    "print(f\"Jammer IDs: {jammer_df['jammerId'].unique()}\")\n",
    "print(f\"Jamming attack sample:\\n{jammer_df.head(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Message Falsification attacks\n",
    "print(\"Message Falsification Details:\")\n",
    "print(f\"Total Falsification records: {len(msg_falsification_df)}\")\n",
    "print(f\"Falsifier IDs: {msg_falsification_df['attackerId'].unique()}\")\n",
    "print(f\"Falsification attack sample:\\n{msg_falsification_df.head(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trust System Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trust score evolution over time\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "for node_id in trust_df['nodeId'].unique()[:5]:  # Plot first 5 nodes\n",
    "    node_data = trust_df[trust_df['nodeId'] == node_id]\n",
    "    plt.plot(node_data['timestamp'], node_data['trustScore'], label=f'Node {node_id}', alpha=0.7)\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Trust Score')\n",
    "plt.title('Trust Score Evolution (First 5 Nodes)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(trust_df['trustScore'], bins=50, alpha=0.7, color='skyblue')\n",
    "plt.xlabel('Trust Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Trust Scores')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "low_trust_nodes = trust_df[trust_df['lowTrustFlag'] == 1]['nodeId'].nunique()\n",
    "high_trust_nodes = trust_df[trust_df['lowTrustFlag'] == 0]['nodeId'].nunique()\n",
    "plt.pie([low_trust_nodes, high_trust_nodes], labels=['Low Trust', 'High Trust'], autopct='%1.1f%%')\n",
    "plt.title('Trust Status Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Nodes with low trust: {low_trust_nodes}\")\n",
    "print(f\"Nodes with high trust: {high_trust_nodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ML Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation matrix\n",
    "feature_cols = ['speed', 'heading', 'interArrivalTime', 'avgPayloadSize']\n",
    "features_subset = features_df[feature_cols].copy()\n",
    "\n",
    "# Drop rows with NaN or infinite values\n",
    "features_subset = features_subset.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "if not features_subset.empty:\n",
    "    correlation_matrix = features_subset.corr()\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=0.5)\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, col in enumerate(feature_cols):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.hist(features_df[col].dropna(), bins=50, alpha=0.7, color=plt.cm.Set3(i/len(feature_cols)))\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Neighbor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neighbor count and distance analysis\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(neighbor_df['neighborCount'], bins=50, alpha=0.7, color='orange')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Neighbor Counts')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(neighbor_df['minDistance'], bins=50, alpha=0.7, color='coral')\n",
    "plt.xlabel('Minimum Distance to Nearest Neighbor')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Minimum Distances')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(neighbor_df['neighborCount'], neighbor_df['minDistance'], alpha=0.5)\n",
    "plt.xlabel('Neighbor Count')\n",
    "plt.ylabel('Minimum Distance')\n",
    "plt.title('Neighbor Count vs Minimum Distance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average neighbor count: {neighbor_df['neighborCount'].mean():.2f}\")\n",
    "print(f\"Average minimum distance: {neighbor_df['minDistance'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ML Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for ML\n",
    "# Create a labeled dataset by merging with attack data\n",
    "\n",
    "# Create a label column based on attack logs\n",
    "features_labeled = features_df.copy()\n",
    "features_labeled['is_attacker'] = 0  # Initialize as non-attacker\n",
    "\n",
    "# Mark nodes that appear in attack logs as attackers\n",
    "attack_nodes = set()\n",
    "if not ddos_df.empty:\n",
    "    attack_nodes.update(ddos_df['attackerId'].unique())\n",
    "if not sybil_df.empty:\n",
    "    attack_nodes.update(sybil_df['attackerId'].unique())\n",
    "if not msg_falsification_df.empty:\n",
    "    attack_nodes.update(msg_falsification_df['attackerId'].unique())\n",
    "\n",
    "# Mark nodes as attackers\n",
    "features_labeled.loc[features_labeled['nodeId'].isin(attack_nodes), 'is_attacker'] = 1\n",
    "\n",
    "# Prepare features for ML model\n",
    "feature_cols_for_ml = ['speed', 'heading', 'interArrivalTime', 'avgPayloadSize']\n",
    "X = features_labeled[feature_cols_for_ml].copy()\n",
    "y = features_labeled['is_attacker'].copy()\n",
    "\n",
    "# Handle infinite values and NaN\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Label distribution - Normal: {(y == 0).sum()}, Attackers: {(y == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Random Forest Results:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "try:\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba_rf)\n",
    "    print(f\"AUC Score: {auc_score:.3f}\")\n",
    "except ValueError:\n",
    "    print(\"AUC Score: Cannot compute (only one class present)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Gradient Boosting classifier\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "y_pred_proba_gb = gb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Gradient Boosting Results:\")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "\n",
    "try:\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba_gb)\n",
    "    print(f\"AUC Score: {auc_score:.3f}\")\n",
    "except ValueError:\n",
    "    print(\"AUC Score: Cannot compute (only one class present)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols_for_ml,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance, x='importance', y='feature')\n",
    "plt.title('Feature Importance for Attack Detection (Random Forest)')\n",
    "plt.xlabel('Importance')\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cross-Validation for Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "rf_scores = cross_val_score(rf_model, X, y, cv=5, scoring='f1')\n",
    "gb_scores = cross_val_score(gb_model, X, y, cv=5, scoring='f1')\n",
    "\n",
    "print(f\"Random Forest CV F1 Scores: {rf_scores}\")\n",
    "print(f\"Random Forest CV F1 Mean: {rf_scores.mean():.3f} (+/- {rf_scores.std() * 2:.3f})\")\n",
    "\n",
    "print(f\"\\nGradient Boosting CV F1 Scores: {gb_scores}\")\n",
    "print(f\"Gradient Boosting CV F1 Mean: {gb_scores.mean():.3f} (+/- {gb_scores.std() * 2:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Detection Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze detection results if available\n",
    "if not detection_df.empty:\n",
    "    print(\"Detection Results Analysis:\")\n",
    "    print(f\"Total detection records: {len(detection_df)}\")\n",
    "    print(f\"Detection types: {detection_df['attackType'].unique()}\")\n",
    "    print(f\"Average detection score: {detection_df['detectionScore'].mean():.3f}\")\n",
    "    \n",
    "    # Plot detection scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for attack_type in detection_df['attackType'].unique():\n",
    "        attack_data = detection_df[detection_df['attackType'] == attack_type]\n",
    "        if not attack_data.empty:\n",
    "            plt.hist(attack_data['detectionScore'], alpha=0.6, label=attack_type, bins=20)\n",
    "    \n",
    "    plt.xlabel('Detection Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Detection Scores by Attack Type')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No detection results available in detection_log.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics\n",
    "print(\"=== VANET SIMULATION SUMMARY ===\")\n",
    "print(f\"Total BSM messages: {len(bsm_df)}\")\n",
    "print(f\"Total vehicles in simulation: {bsm_df['nodeId'].nunique()}\")\n",
    "print(f\"Simulation duration: {bsm_df['timestamp'].max() - bsm_df['timestamp'].min():.2f} seconds\")\n",
    "\n",
    "print(f\"\\nAttacks detected:\")\n",
    "if not ddos_df.empty:\n",
    "    print(f\"  - DDoS attacks: {ddos_df['attackerId'].nunique()} malicious nodes\")\n",
    "if not sybil_df.empty:\n",
    "    print(f\"  - Sybil attacks: {sybil_df['attackerId'].nunique()} malicious nodes\")\n",
    "if not jammer_df.empty:\n",
    "    print(f\"  - Jamming attacks: {jammer_df['jammerId'].nunique()} jammer nodes\")\n",
    "if not msg_falsification_df.empty:\n",
    "    print(f\"  - Message falsification: {msg_falsification_df['attackerId'].nunique()} malicious nodes\")\n",
    "\n",
    "print(f\"\\nTrust system effectiveness:\")\n",
    "if not trust_df.empty:\n",
    "    avg_trust = trust_df['trustScore'].mean()\n",
    "    low_trust_count = trust_df[trust_df['lowTrustFlag'] == 1]['nodeId'].nunique()\n",
    "    print(f\"  - Average trust score: {avg_trust:.3f}\")\n",
    "    print(f\"  - Nodes flagged as low trust: {low_trust_count}\")\n",
    "\n",
    "print(f\"\\nML Model Performance:\")\n",
    "if 'rf_scores' in locals():\n",
    "    print(f\"  - Random Forest F1 Score: {rf_scores.mean():.3f}\")\n",
    "if 'gb_scores' in locals():\n",
    "    print(f\"  - Gradient Boosting F1 Score: {gb_scores.mean():.3f}\")\n",
    "\n",
    "print(f\"\\n=== END SUMMARY ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}