{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6444f743",
   "metadata": {},
   "source": [
    "\n",
    "# HISOL Multi-Scenario Analysis \u2014 Extended\n",
    "\n",
    "This notebook builds on the basic multi-scenario loader and adds:\n",
    "\n",
    "- Feature extraction for ML (per-window aggregation)\n",
    "- Supervised ML pipeline (Random Forest) for attack detection (Sybil/Replay/Jammer)\n",
    "- Evaluation metrics: Accuracy, Precision, Recall, F1, ROC/AUC\n",
    "- Trust score computation and visualization (simple exponential smoothing trust model)\n",
    "- Per-scenario aggregation and plots ready for inclusion in Section 4\n",
    "\n",
    "**Assumptions / Input directory**\n",
    "\n",
    "Root dataset path:  \n",
    "`/home/jeanhuit/Documents/Thesis/results`\n",
    "\n",
    "Directory layout:\n",
    "- `/home/jeanhuit/Documents/Thesis/results/highway/density-50/run-1/...`\n",
    "- `/home/jeanhuit/Documents/Thesis/results/highway/density-100/run-1/...`\n",
    "- `/home/jeanhuit/Documents/Thesis/results/highway/density-150/run-1/...`\n",
    "- same for `mixed` and `urban`\n",
    "\n",
    "Each run directory must contain:\n",
    "- `bsm_log.csv`, `rssi_log.csv`, `neighbor_log.csv`, `sybil_log.csv`, `replay_log.csv`, `jammer_log.csv`\n",
    "\n",
    "If some files are missing the notebook will skip them and continue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0575be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports and helpers\n",
    "import os, glob, json, math\n",
    "import pandas as pd, numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# display helper (ace_tools available in environment when running via tool)\n",
    "try:\n",
    "    from ace_tools import display_dataframe_to_user\n",
    "except:\n",
    "    def display_dataframe_to_user(name, df):\n",
    "        print(f\"=== {name} ===\")\n",
    "        print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b66ae0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 9 runs\n",
      "highway density-50 files: ['bsm_log.csv', 'rssi_log.csv', 'neighbor_log.csv', 'sybil_log.csv', 'replay_log.csv', 'jammer_log.csv']\n",
      "highway density-100 files: ['bsm_log.csv', 'rssi_log.csv', 'neighbor_log.csv', 'sybil_log.csv', 'replay_log.csv', 'jammer_log.csv']\n",
      "highway density-150 files: ['bsm_log.csv', 'rssi_log.csv', 'neighbor_log.csv', 'sybil_log.csv', 'replay_log.csv', 'jammer_log.csv']\n",
      "mixed density-50 files: ['bsm_log.csv', 'rssi_log.csv', 'neighbor_log.csv', 'sybil_log.csv', 'replay_log.csv', 'jammer_log.csv']\n",
      "mixed density-100 files: ['bsm_log.csv', 'rssi_log.csv', 'neighbor_log.csv', 'sybil_log.csv', 'replay_log.csv', 'jammer_log.csv']\n",
      "mixed density-150 files: ['bsm_log.csv', 'rssi_log.csv', 'neighbor_log.csv', 'sybil_log.csv', 'replay_log.csv', 'jammer_log.csv']\n",
      "urban density-50 files: ['bsm_log.csv', 'rssi_log.csv', 'neighbor_log.csv', 'sybil_log.csv', 'replay_log.csv', 'jammer_log.csv']\n",
      "urban density-100 files: ['bsm_log.csv', 'rssi_log.csv', 'neighbor_log.csv', 'sybil_log.csv', 'replay_log.csv', 'jammer_log.csv']\n",
      "urban density-150 files: ['bsm_log.csv', 'rssi_log.csv', 'neighbor_log.csv', 'sybil_log.csv', 'replay_log.csv', 'jammer_log.csv']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ROOT = r\"/home/jeanhuit/Documents/Workspace/simulation/results\"\n",
    "scenario_dirs = ['highway','mixed','urban']\n",
    "densities = ['density-50','density-100','density-150']\n",
    "\n",
    "runs = []\n",
    "for scen in scenario_dirs:\n",
    "    for dens in densities:\n",
    "        runpath = os.path.join(ROOT, scen, dens, \"run-1\")\n",
    "        if os.path.exists(runpath):\n",
    "            files = {f: os.path.join(runpath, f) for f in [\n",
    "                'bsm_log.csv','rssi_log.csv','neighbor_log.csv',\n",
    "                'sybil_log.csv','replay_log.csv','jammer_log.csv'\n",
    "            ] if os.path.exists(os.path.join(runpath,f))}\n",
    "            runs.append({'scenario':scen,'density':dens,'path':runpath,'files':files})\n",
    "print(\"Discovered\", len(runs), \"runs\")\n",
    "for r in runs:\n",
    "    print(r['scenario'], r['density'], \"files:\", list(r['files'].keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5efb973",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_run_files(run):\n",
    "    data = {}\n",
    "    for name,path in run['files'].items():\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            data[name] = df\n",
    "        except Exception as e:\n",
    "            print(\"Error loading\", path, e)\n",
    "            data[name] = None\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "154d4a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature extraction parameters\n",
    "WINDOW = 5.0  # seconds per aggregation window\n",
    "MIN_MESSAGES_PER_WINDOW = 1\n",
    "\n",
    "def extract_features_from_run(run):\n",
    "    data = load_run_files(run)\n",
    "    bsm = data.get('bsm_log.csv')\n",
    "    rssi = data.get('rssi_log.csv')\n",
    "    neighbor = data.get('neighbor_log.csv')\n",
    "    sybil = data.get('sybil_log.csv')\n",
    "    replay = data.get('replay_log.csv')\n",
    "    jammer = data.get('jammer_log.csv')\n",
    "\n",
    "    # parse bsm: expect columns node,x,y,vx,vy,time (if not, try infer)\n",
    "    if bsm is None:\n",
    "        return None\n",
    "    else:\n",
    "        # Based on data analysis, take first 6 columns and map to [time, x, y, vx, vy, node]\n",
    "        bsm_df = bsm.iloc[:,0:6].copy()\n",
    "        bsm_df.columns = ['time', 'x', 'y', 'vx', 'vy', 'node']\n",
    "        bsm_df = bsm.iloc[:,0:6].copy()\n",
    "        bsm_df.columns = ['node','x','y','vx','vy','time']\n",
    "    bsm_df['time'] = pd.to_numeric(bsm_df['time'], errors='coerce')\n",
    "    bsm_df['node'] = bsm_df['node'].astype(int)\n",
    "\n",
    "    # parse rssi: node,msg,rssi or node,rawmsg,rssi\n",
    "    rssi_df = None\n",
    "    if rssi is not None:\n",
    "        cols = list(rssi.columns)\n",
    "        if len(cols) >= 3:\n",
    "            tmp = rssi.iloc[:,0:3].copy()\n",
    "            tmp.columns = ['node','msg','rssi']\n",
    "            tmp['node'] = tmp['node'].astype(int)\n",
    "            tmp['rssi'] = pd.to_numeric(tmp['rssi'], errors='coerce')\n",
    "            # extract sender id from msg if possible\n",
    "            def get_sender(msg):\n",
    "                try:\n",
    "                    if isinstance(msg, str) and msg.startswith(\"BSM,\"):\n",
    "                        return int(msg.split(\",\")[1])\n",
    "                except:\n",
    "                    return np.nan\n",
    "                return np.nan\n",
    "            tmp['sender'] = tmp['msg'].apply(get_sender)\n",
    "            rssi_df = tmp\n",
    "    # parse neighbor: time,node,count (if available)\n",
    "    neighbor_df = None\n",
    "    if neighbor is not None:\n",
    "        nd = neighbor.copy()\n",
    "        if nd.shape[1] >= 3:\n",
    "            nd = nd.iloc[:,0:3]\n",
    "            nd.columns = ['time','node','count']\n",
    "            nd['time'] = pd.to_numeric(nd['time'], errors='coerce')\n",
    "            nd['node'] = nd['node'].astype(int)\n",
    "            nd['count'] = pd.to_numeric(nd['count'], errors='coerce')\n",
    "            neighbor_df = nd\n",
    "\n",
    "    # Build time windows\n",
    "    t_min = bsm_df['time'].min()\n",
    "    t_max = bsm_df['time'].max()\n",
    "    windows = np.arange(t_min, t_max+WINDOW, WINDOW)\n",
    "\n",
    "    features = []\n",
    "    labels = []  # 0 benign, 1 sybil/replay/jam (we'll label windows that have attacks)\n",
    "\n",
    "    # gather attack times if present\n",
    "    attack_times = []\n",
    "    if sybil is not None:\n",
    "        # try extract numeric times from first column\n",
    "        try:\n",
    "            attack_times += list(pd.to_numeric(sybil.iloc[:,0], errors='coerce').dropna().unique())\n",
    "        except:\n",
    "            pass\n",
    "    if replay is not None:\n",
    "        try:\n",
    "            attack_times += list(pd.to_numeric(replay.iloc[:,0], errors='coerce').dropna().unique())\n",
    "        except:\n",
    "            pass\n",
    "    if jammer is not None:\n",
    "        try:\n",
    "            attack_times += list(pd.to_numeric(jammer.iloc[:,0], errors='coerce').dropna().unique())\n",
    "        except:\n",
    "            pass\n",
    "    attack_times = sorted([float(x) for x in attack_times if not pd.isna(x)])\n",
    "\n",
    "    # For each window compute features\n",
    "    for i in range(len(windows)-1):\n",
    "        t0 = windows[i]\n",
    "        t1 = windows[i+1]\n",
    "        win_bsm = bsm_df[(bsm_df['time'] >= t0) & (bsm_df['time'] < t1)]\n",
    "        if win_bsm.empty:\n",
    "            continue\n",
    "        # feature: total messages, unique senders, mean speed, var speed\n",
    "        total_msgs = len(win_bsm)\n",
    "        unique_senders = win_bsm['node'].nunique()\n",
    "        speeds = np.sqrt(win_bsm['vx']**2 + win_bsm['vy']**2)\n",
    "        mean_speed = speeds.mean()\n",
    "        var_speed = speeds.var()\n",
    "        # neighbor features if available: mean neighbor count in window\n",
    "        mean_neighbors = np.nan\n",
    "        if neighbor_df is not None:\n",
    "            nd = neighbor_df[(neighbor_df['time'] >= t0) & (neighbor_df['time'] < t1)]\n",
    "            if not nd.empty:\n",
    "                mean_neighbors = nd['count'].mean()\n",
    "        # rssi features: mean rssi for messages in window\n",
    "        mean_rssi = np.nan\n",
    "        if rssi_df is not None:\n",
    "            # match rssi sender to window senders\n",
    "            # some rssi entries may not have valid sender; dropna\n",
    "            r = rssi_df[(rssi_df['sender'].notna()) & (rssi_df['sender'].isin(win_bsm['node'].unique()))]\n",
    "            if not r.empty:\n",
    "                mean_rssi = r['rssi'].mean()\n",
    "        # label: if any attack time falls inside window -> label 1 else 0\n",
    "        has_attack = any((t >= t0) and (t < t1) for t in attack_times)\n",
    "        features.append({\n",
    "            't0': t0, 't1': t1,\n",
    "            'total_msgs': total_msgs,\n",
    "            'unique_senders': unique_senders,\n",
    "            'mean_speed': mean_speed,\n",
    "            'var_speed': var_speed,\n",
    "            'mean_neighbors': mean_neighbors,\n",
    "            'mean_rssi': mean_rssi,\n",
    "            'scenario': run['scenario'],\n",
    "            'density': run['density']\n",
    "        })\n",
    "        labels.append(1 if has_attack else 0)\n",
    "    if len(features)==0:\n",
    "        return None\n",
    "    X = pd.DataFrame(features)\n",
    "    y = pd.Series(labels, name='label')\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91aa9a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows: 1620\n",
      "=== features_sample ===\n",
      "     t0    t1  total_msgs  unique_senders  mean_speed  var_speed  \\\n",
      "0   1.0   6.0        2499              50        25.0        0.0   \n",
      "1   6.0  11.0        2500              50        25.0        0.0   \n",
      "2  11.0  16.0        2500              50        25.0        0.0   \n",
      "3  16.0  21.0        2500              50        25.0        0.0   \n",
      "4  21.0  26.0        2500              50        25.0        0.0   \n",
      "\n",
      "   mean_neighbors  mean_rssi scenario     density  \n",
      "0            49.0        NaN  highway  density-50  \n",
      "1            49.0        NaN  highway  density-50  \n",
      "2            49.0        NaN  highway  density-50  \n",
      "3            49.0        NaN  highway  density-50  \n",
      "4            49.0        NaN  highway  density-50  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build dataset across runs\n",
    "dataset_X = []\n",
    "dataset_y = []\n",
    "meta = []\n",
    "\n",
    "for run in runs:\n",
    "    rv = extract_features_from_run(run)\n",
    "    if rv is None:\n",
    "        print(\"No features for\", run['scenario'], run['density'])\n",
    "        continue\n",
    "    X,y = rv\n",
    "    dataset_X.append(X)\n",
    "    dataset_y.append(y)\n",
    "    meta.append({'scenario': run['scenario'], 'density': run['density'], 'path': run['path']})\n",
    "\n",
    "if len(dataset_X)==0:\n",
    "    print(\"No data found across runs\")\n",
    "else:\n",
    "    X_all = pd.concat(dataset_X, ignore_index=True)\n",
    "    y_all = pd.concat(dataset_y, ignore_index=True)\n",
    "    print(\"Total windows:\", len(X_all))\n",
    "    display_dataframe_to_user(\"features_sample\", X_all.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c038021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeanhuit/Documents/Workspace/Env/ml/lib/python3.13/site-packages/numpy/lib/_nanfunctions_impl.py:1214: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics: {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0}\n",
      "Saved model and scaler to /home/jeanhuit/Documents/Workspace/simulation/models\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocess: fillna and simple imputation\n",
    "X = X_all.copy()\n",
    "X['mean_neighbors'] = X['mean_neighbors'].fillna(X['mean_neighbors'].median())\n",
    "X['mean_rssi'] = X['mean_rssi'].fillna(X['mean_rssi'].median())\n",
    "\n",
    "feature_cols = ['total_msgs','unique_senders','mean_speed','var_speed','mean_neighbors','mean_rssi']\n",
    "Xf = X[feature_cols].fillna(0)\n",
    "y = y_all.values\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xf, y, test_size=0.2, random_state=42, stratify=y if len(set(y))>1 else None)\n",
    "\n",
    "# scale\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "# train model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_s, y_train)\n",
    "\n",
    "# evaluate\n",
    "y_pred = clf.predict(X_test_s)\n",
    "# evaluate\n",
    "y_pred = clf.predict(X_test_s)\n",
    "# Fix for IndexError: check number of probability columns\n",
    "if hasattr(clf, 'predict_proba'):\n",
    "    prob_pred = clf.predict_proba(X_test_s)\n",
    "    if prob_pred.shape[1] > 1:\n",
    "        y_prob = prob_pred[:, 1]\n",
    "    else:\n",
    "        # If only one class in prediction, use probabilities as-is\n",
    "        # For evaluation metrics, make predictions reflect the single class issue\n",
    "        y_prob = prob_pred[:, 0]\n",
    "else:\n",
    "    y_prob = None\n",
    "\n",
    "metrics = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred),\n",
    "    'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "    'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "    'f1': f1_score(y_test, y_pred, zero_division=0)\n",
    "}\n",
    "if y_prob is not None and len(set(y_test))>1:\n",
    "    metrics['roc_auc'] = roc_auc_score(y_test, y_prob)\n",
    "print(\"Evaluation metrics:\", metrics)\n",
    "\n",
    "# save model and scaler\n",
    "model_dir = \"/home/jeanhuit/Documents/Workspace/simulation/models\"\n",
    "import os\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "joblib.dump(clf, os.path.join(model_dir, \"rf_hisol.joblib\"))\n",
    "joblib.dump(scaler, os.path.join(model_dir, \"scaler_hisol.joblib\"))\n",
    "print(\"Saved model and scaler to\", model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84ec7bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: \"node\" column not found in BSM data\n",
      "Available columns: ['0', '25', '0.1', '25.1', '0.2', '1']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Trust model: exponential smoothing per node based on neighbor consistency\n",
    "# trust_t+1 = alpha * trust_t + (1-alpha) * evidence\n",
    "alpha = 0.8\n",
    "trust_threshold = 0.4\n",
    "\n",
    "def compute_trust_over_time(run):\n",
    "    data = load_run_files(run)\n",
    "    bsm = data.get('bsm_log.csv')\n",
    "    neighbor = data.get('neighbor_log.csv')\n",
    "    if bsm is None:\n",
    "        return None\n",
    "    # prepare per-node trust timeline\n",
    "    # Check if required columns exist\n",
    "    if 'node' not in bsm.columns:\n",
    "        print('Error: \"node\" column not found in BSM data')\n",
    "        print('Available columns:', list(bsm.columns))\n",
    "        return None\n",
    "    nodes = sorted(bsm['node'].unique())\n",
    "    # Also check for 'time' column\n",
    "    if 'time' not in bsm.columns:\n",
    "        print('Error: \"time\" column not found in BSM data')\n",
    "        return None\n",
    "    t_min = bsm['time'].min()\n",
    "    t_max = bsm['time'].max()\n",
    "    windows = np.arange(t_min, t_max+5.0, 5.0)\n",
    "    # initialize trust\n",
    "    trust = {n: 1.0 for n in nodes}\n",
    "    trust_records = []\n",
    "    for i in range(len(windows)-1):\n",
    "        t0 = windows[i]; t1=windows[i+1]\n",
    "        win_bsm = bsm[(bsm['time']>=t0)&(bsm['time']<t1)]\n",
    "        # evidence: per node, fraction of messages with inconsistent position compared to neighbors\n",
    "        evidence = {}\n",
    "        for n in nodes:\n",
    "            evidence[n]=0.0\n",
    "        for idx,row in win_bsm.iterrows():\n",
    "            n = row['node']\n",
    "            # neighbors approx: messages from other nodes in same window\n",
    "            others = win_bsm[win_bsm['node']!=n]\n",
    "            if others.empty:\n",
    "                continue\n",
    "            # compute avg position diff\n",
    "            dists = np.sqrt((others['x']-row['x'])**2 + (others['y']-row['y'])**2)\n",
    "            # if distance to median neighbor position > threshold, mark as inconsistent\n",
    "            median_dist = dists.median()\n",
    "            if median_dist > 100.0: # threshold in meters\n",
    "                evidence[n] += 1\n",
    "        # normalize evidence per node by number of messages\n",
    "        for n in nodes:\n",
    "            count_n = len(win_bsm[win_bsm['node']==n])\n",
    "            e = evidence[n] / count_n if count_n>0 else 0.0\n",
    "            # update trust\n",
    "            trust[n] = alpha*trust[n] + (1-alpha)*(1 - min(1.0,e)) # higher evidence reduces trust\n",
    "        # record snapshot\n",
    "        snapshot = {'time': t1}\n",
    "        for n in nodes:\n",
    "            snapshot[f\"trust_{n}\"] = trust[n]\n",
    "        trust_records.append(snapshot)\n",
    "    df_trust = pd.DataFrame(trust_records)\n",
    "    return df_trust\n",
    "\n",
    "# compute trust for first run as example\n",
    "if len(runs)>0:\n",
    "    tr = compute_trust_over_time(runs[0])\n",
    "    if tr is not None:\n",
    "        display_dataframe_to_user(\"trust_sample\", tr.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0468f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example plots: ROC curve and trust over time for first run\n",
    "import matplotlib.pyplot as plt\n",
    "fig_dir = \"/home/jeanhuit/Documents/Workspace/simulation/figures\"\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "# ROC\n",
    "if 'y_prob' in globals() and y_prob is not None and len(set(y_test))>1:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='RF')\n",
    "    plt.plot([0,1],[0,1],'k--')\n",
    "    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC curve')\n",
    "    plt.legend()\n",
    "    roc_path = os.path.join(fig_dir, \"roc_curve.png\")\n",
    "    plt.savefig(roc_path)\n",
    "    print(\"Saved ROC to\", roc_path)\n",
    "\n",
    "# Trust plot for sample run\n",
    "if 'tr' in globals() and tr is not None:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    # plot trust for first 5 nodes\n",
    "    cols = [c for c in tr.columns if c.startswith('trust_')][:5]\n",
    "    for c in cols:\n",
    "        plt.plot(tr['time'], tr[c], label=c)\n",
    "    plt.xlabel('time (s)'); plt.ylabel('trust'); plt.title('Trust over time (sample)')\n",
    "    plt.legend()\n",
    "    trust_path = os.path.join(fig_dir, \"trust_sample.png\")\n",
    "    plt.savefig(trust_path)\n",
    "    print(\"Saved trust plot to\", trust_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c87e1a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features and labels to /home/jeanhuit/Documents/Workspace/simulation/output\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save aggregated window-level features and labels\n",
    "outdir = \"/home/jeanhuit/Documents/Workspace/simulation/output\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "X_all.to_csv(os.path.join(outdir, \"window_features.csv\"), index=False)\n",
    "pd.Series(y_all, name='label').to_csv(os.path.join(outdir, \"window_labels.csv\"), index=False)\n",
    "print(\"Saved features and labels to\", outdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3387cf",
   "metadata": {},
   "source": [
    "\n",
    "## Next steps\n",
    "\n",
    "- If you want per-run ROC/metrics, we can loop over each run and compute model performance separately.\n",
    "- We can add cross-validation and hyperparameter tuning for the Random Forest.\n",
    "- For trust, you can adjust the evidence function (e.g., incorporate RSSI variance, neighbor_count variations).\n",
    "- If you'd like, I can convert key figures and summary tables into LaTeX-ready format for Section 4.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}