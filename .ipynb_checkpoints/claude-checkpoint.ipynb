{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be70bb96-cbd3-44e4-bebd-1a3a8c15780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V2X Simulation ML Analysis Notebook\n",
    "# Complete Machine Learning Pipeline for Attack Detection in V2X Networks\n",
    "\n",
    "\"\"\"\n",
    "This notebook performs comprehensive ML analysis on V2X simulation data:\n",
    "- Multi-scenario analysis (highway, urban, mixed)\n",
    "- Feature engineering with time-windowing\n",
    "- Supervised ML for attack detection\n",
    "- Trust score computation\n",
    "- Complete evaluation and visualization\n",
    "\n",
    "Author: Generated for V2X Security Research\n",
    "Date: 2025\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 1. SETUP & CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n",
    ")\n",
    "import joblib\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Root directory - CHANGE THIS TO YOUR DATA LOCATION\n",
    "ROOT_DIR = Path(\"/home/jeanhuit/Documents/Workspace/simulation/results/\")\n",
    "\n",
    "# Output directories\n",
    "FIG_DIR = Path(\"/home/jeanhuit/Documents/Workspace/simulation/figures\")\n",
    "OUTPUT_DIR = Path(\"/home/jeanhuit/Documents/Workspace/simulation/output\")\n",
    "MODEL_PATH = Path(\"/home/jeanhuit/Documents/Workspace/simulation/model\")\n",
    "\n",
    "# Create output directories\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Scenarios and densities\n",
    "SCENARIOS = ['highway', 'mixed', 'urban']\n",
    "DENSITIES = ['density-50', 'density-100','density-150']\n",
    "RUNS = ['run-1']  # Extend with ['run-1', 'run-2', ...] for multi-run\n",
    "\n",
    "# Feature engineering parameters\n",
    "WINDOW_SIZE = 5.0  # seconds\n",
    "TRUST_ALPHA = 0.7  # Exponential smoothing parameter\n",
    "\n",
    "# ML parameters\n",
    "TEST_SIZE = 0.3\n",
    "RANDOM_STATE = 42\n",
    "CV_FOLDS = 5\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"V2X SIMULATION ML ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Root Directory: {ROOT_DIR}\")\n",
    "print(f\"Scenarios: {SCENARIOS}\")\n",
    "print(f\"Densities: {DENSITIES}\")\n",
    "print(f\"Window Size: {WINDOW_SIZE}s\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# 2. DATA LOADING & NETWORK STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "def load_scenario_data(scenario, density, run='run-1'):\n",
    "    \"\"\"\n",
    "    Load all CSV files for a specific scenario/density/run.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing all loaded dataframes\n",
    "    \"\"\"\n",
    "    base_path = ROOT_DIR / scenario / str(density) / run\n",
    "    \n",
    "    data = {}\n",
    "    files = ['bsm_log.csv', 'rssi_log.csv', 'neighbor_log.csv', \n",
    "             'sybil_log.csv', 'replay_log.csv', 'jammer_log.csv']\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = base_path / file\n",
    "        if file_path.exists():\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                data[file.replace('.csv', '')] = df\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}\")\n",
    "                data[file.replace('.csv', '')] = pd.DataFrame()\n",
    "        else:\n",
    "            print(f\"Warning: {file_path} not found\")\n",
    "            data[file.replace('.csv', '')] = pd.DataFrame()\n",
    "    \n",
    "    return data\n",
    "\n",
    "def compute_network_statistics(data_dict):\n",
    "    \"\"\"\n",
    "    Compute basic network statistics from loaded data.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Network statistics\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    bsm = data_dict.get('bsm_log', pd.DataFrame())\n",
    "    if not bsm.empty:\n",
    "        stats['total_sent'] = len(bsm[bsm['type'] == 'sent']) if 'type' in bsm.columns else 0\n",
    "        stats['total_received'] = len(bsm[bsm['type'] == 'received']) if 'type' in bsm.columns else 0\n",
    "        stats['pdr'] = stats['total_received'] / stats['total_sent'] if stats['total_sent'] > 0 else 0\n",
    "    else:\n",
    "        stats['total_sent'] = 0\n",
    "        stats['total_received'] = 0\n",
    "        stats['pdr'] = 0\n",
    "    \n",
    "    neighbor = data_dict.get('neighbor_log', pd.DataFrame())\n",
    "    if not neighbor.empty and 'vehicle_id' in neighbor.columns:\n",
    "        stats['avg_neighbors'] = neighbor.groupby('vehicle_id').size().mean()\n",
    "        stats['unique_vehicles'] = neighbor['vehicle_id'].nunique()\n",
    "    else:\n",
    "        stats['avg_neighbors'] = 0\n",
    "        stats['unique_vehicles'] = 0\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Load all data\n",
    "print(\"\\n[1/10] Loading data from all scenarios...\")\n",
    "all_data = {}\n",
    "network_stats = []\n",
    "\n",
    "for scenario in tqdm(SCENARIOS, desc=\"Scenarios\"):\n",
    "    for density in DENSITIES:\n",
    "        for run in RUNS:\n",
    "            key = f\"{scenario}_{density}_{run}\"\n",
    "            try:\n",
    "                data = load_scenario_data(scenario, density, run)\n",
    "                all_data[key] = data\n",
    "                \n",
    "                stats = compute_network_statistics(data)\n",
    "                stats.update({\n",
    "                    'scenario': scenario,\n",
    "                    'density': density,\n",
    "                    'run': run\n",
    "                })\n",
    "                network_stats.append(stats)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {key}: {e}\")\n",
    "\n",
    "# Create network statistics table\n",
    "network_stats_df = pd.DataFrame(network_stats)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NETWORK STATISTICS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(network_stats_df.to_string())\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save network statistics\n",
    "network_stats_df.to_csv(OUTPUT_DIR / \"network_statistics.csv\", index=False)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "def create_time_windows(df, time_col='time', window_size=5.0):\n",
    "    \"\"\"\n",
    "    Create time window indices for a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with time column\n",
    "        time_col: Name of time column\n",
    "        window_size: Window size in seconds\n",
    "    \n",
    "    Returns:\n",
    "        Series: Window index for each row\n",
    "    \"\"\"\n",
    "    if df.empty or time_col not in df.columns:\n",
    "        return pd.Series([], dtype=int)\n",
    "    \n",
    "    return (df[time_col] // window_size).astype(int)\n",
    "\n",
    "def extract_message_features(bsm_df, window_size=5.0):\n",
    "    \"\"\"\n",
    "    Extract message-based features from BSM logs.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Features per vehicle per window\n",
    "    \"\"\"\n",
    "    if bsm_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    required_cols = ['time', 'vehicle_id']\n",
    "    if not all(col in bsm_df.columns for col in required_cols):\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    bsm_df['window'] = create_time_windows(bsm_df, window_size=window_size)\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    for (vehicle_id, window), group in bsm_df.groupby(['vehicle_id', 'window']):\n",
    "        feat = {\n",
    "            'vehicle_id': vehicle_id,\n",
    "            'window': window,\n",
    "            'time_start': window * window_size,\n",
    "            'msg_count_total': len(group),\n",
    "        }\n",
    "        \n",
    "        # Sent/received counts\n",
    "        if 'type' in group.columns:\n",
    "            feat['msg_count_sent'] = len(group[group['type'] == 'sent'])\n",
    "            feat['msg_count_received'] = len(group[group['type'] == 'received'])\n",
    "        \n",
    "        # Unique senders\n",
    "        if 'sender_id' in group.columns:\n",
    "            feat['unique_senders'] = group['sender_id'].nunique()\n",
    "        \n",
    "        # Speed statistics\n",
    "        if 'speed' in group.columns:\n",
    "            feat['speed_mean'] = group['speed'].mean()\n",
    "            feat['speed_std'] = group['speed'].std()\n",
    "            feat['speed_min'] = group['speed'].min()\n",
    "            feat['speed_max'] = group['speed'].max()\n",
    "        \n",
    "        features.append(feat)\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "def extract_neighbor_features(neighbor_df, window_size=5.0):\n",
    "    \"\"\"\n",
    "    Extract neighbor-based features.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Neighbor features per vehicle per window\n",
    "    \"\"\"\n",
    "    if neighbor_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    required_cols = ['time', 'vehicle_id']\n",
    "    if not all(col in neighbor_df.columns for col in required_cols):\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    neighbor_df['window'] = create_time_windows(neighbor_df, window_size=window_size)\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    for (vehicle_id, window), group in neighbor_df.groupby(['vehicle_id', 'window']):\n",
    "        feat = {\n",
    "            'vehicle_id': vehicle_id,\n",
    "            'window': window,\n",
    "            'neighbor_count': len(group),\n",
    "        }\n",
    "        \n",
    "        if 'neighbor_id' in group.columns:\n",
    "            feat['unique_neighbors'] = group['neighbor_id'].nunique()\n",
    "        \n",
    "        if 'duration' in group.columns:\n",
    "            feat['neighbor_duration_mean'] = group['duration'].mean()\n",
    "            feat['neighbor_duration_std'] = group['duration'].std()\n",
    "            feat['neighbor_duration_max'] = group['duration'].max()\n",
    "        \n",
    "        features.append(feat)\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "def extract_rssi_features(rssi_df, window_size=5.0):\n",
    "    \"\"\"\n",
    "    Extract RSSI-based features.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: RSSI features per vehicle per window\n",
    "    \"\"\"\n",
    "    if rssi_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    required_cols = ['time', 'vehicle_id', 'rssi']\n",
    "    if not all(col in rssi_df.columns for col in required_cols):\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    rssi_df['window'] = create_time_windows(rssi_df, window_size=window_size)\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    for (vehicle_id, window), group in rssi_df.groupby(['vehicle_id', 'window']):\n",
    "        rssi_values = group['rssi'].values\n",
    "        \n",
    "        feat = {\n",
    "            'vehicle_id': vehicle_id,\n",
    "            'window': window,\n",
    "            'rssi_mean': rssi_values.mean(),\n",
    "            'rssi_std': rssi_values.std(),\n",
    "            'rssi_min': rssi_values.min(),\n",
    "            'rssi_max': rssi_values.max(),\n",
    "            'rssi_variance': rssi_values.var(),\n",
    "        }\n",
    "        \n",
    "        # RSSI trend (linear regression slope)\n",
    "        if len(rssi_values) > 1:\n",
    "            x = np.arange(len(rssi_values))\n",
    "            slope, _ = np.polyfit(x, rssi_values, 1)\n",
    "            feat['rssi_trend'] = slope\n",
    "        else:\n",
    "            feat['rssi_trend'] = 0\n",
    "        \n",
    "        features.append(feat)\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "def extract_spectral_features(bsm_df, window_size=5.0):\n",
    "    \"\"\"\n",
    "    Extract spectral features from message intervals using FFT.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Spectral features per vehicle per window\n",
    "    \"\"\"\n",
    "    if bsm_df.empty or 'time' not in bsm_df.columns:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    bsm_df['window'] = create_time_windows(bsm_df, window_size=window_size)\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    for (vehicle_id, window), group in bsm_df.groupby(['vehicle_id', 'window']):\n",
    "        times = np.sort(group['time'].values)\n",
    "        \n",
    "        if len(times) > 2:\n",
    "            intervals = np.diff(times)\n",
    "            \n",
    "            # FFT of intervals\n",
    "            fft_vals = np.abs(fft(intervals))\n",
    "            \n",
    "            feat = {\n",
    "                'vehicle_id': vehicle_id,\n",
    "                'window': window,\n",
    "                'spectral_energy': np.sum(fft_vals**2),\n",
    "                'spectral_entropy': stats.entropy(fft_vals + 1e-10),\n",
    "                'interval_mean': intervals.mean(),\n",
    "                'interval_std': intervals.std(),\n",
    "            }\n",
    "            features.append(feat)\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "def combine_all_features(data_dict, window_size=5.0):\n",
    "    \"\"\"\n",
    "    Extract and combine all features for a scenario.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Combined feature matrix\n",
    "    \"\"\"\n",
    "    # Extract features from each log\n",
    "    msg_feat = extract_message_features(data_dict.get('bsm_log', pd.DataFrame()), window_size)\n",
    "    neighbor_feat = extract_neighbor_features(data_dict.get('neighbor_log', pd.DataFrame()), window_size)\n",
    "    rssi_feat = extract_rssi_features(data_dict.get('rssi_log', pd.DataFrame()), window_size)\n",
    "    spectral_feat = extract_spectral_features(data_dict.get('bsm_log', pd.DataFrame()), window_size)\n",
    "    \n",
    "    # Merge all features\n",
    "    features = msg_feat\n",
    "    \n",
    "    if not neighbor_feat.empty:\n",
    "        features = features.merge(neighbor_feat, on=['vehicle_id', 'window'], how='outer')\n",
    "    \n",
    "    if not rssi_feat.empty:\n",
    "        features = features.merge(rssi_feat, on=['vehicle_id', 'window'], how='outer')\n",
    "    \n",
    "    if not spectral_feat.empty:\n",
    "        features = features.merge(spectral_feat, on=['vehicle_id', 'window'], how='outer')\n",
    "    \n",
    "    return features.fillna(0)\n",
    "\n",
    "print(\"\\n[2/10] Extracting features from all scenarios...\")\n",
    "\n",
    "all_features = {}\n",
    "\n",
    "for key, data in tqdm(all_data.items(), desc=\"Feature Extraction\"):\n",
    "    try:\n",
    "        features = combine_all_features(data, window_size=WINDOW_SIZE)\n",
    "        if not features.empty:\n",
    "            scenario, density, run = key.split('_')\n",
    "            features['scenario'] = scenario\n",
    "            features['density'] = int(density)\n",
    "            features['run'] = run\n",
    "            all_features[key] = features\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting features for {key}: {e}\")\n",
    "\n",
    "print(f\"✓ Extracted features for {len(all_features)} configurations\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. LABEL GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "def generate_labels(data_dict, window_size=5.0):\n",
    "    \"\"\"\n",
    "    Generate labels from attack logs.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Labels per vehicle per window\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    \n",
    "    # Combine all attack logs\n",
    "    attack_dfs = []\n",
    "    attack_types = []\n",
    "    \n",
    "    for attack_type in ['sybil_log', 'replay_log', 'jammer_log']:\n",
    "        df = data_dict.get(attack_type, pd.DataFrame())\n",
    "        if not df.empty and 'time' in df.columns and 'vehicle_id' in df.columns:\n",
    "            df['attack_type'] = attack_type.replace('_log', '')\n",
    "            attack_dfs.append(df)\n",
    "    \n",
    "    if not attack_dfs:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    attacks = pd.concat(attack_dfs, ignore_index=True)\n",
    "    attacks['window'] = create_time_windows(attacks, window_size=window_size)\n",
    "    \n",
    "    # Get all vehicle-window combinations from attacks\n",
    "    for (vehicle_id, window), group in attacks.groupby(['vehicle_id', 'window']):\n",
    "        labels.append({\n",
    "            'vehicle_id': vehicle_id,\n",
    "            'window': window,\n",
    "            'is_malicious': 1,\n",
    "            'attack_type': group['attack_type'].iloc[0]  # Primary attack type\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(labels)\n",
    "\n",
    "print(\"\\n[3/10] Generating labels...\")\n",
    "\n",
    "all_labels = {}\n",
    "\n",
    "for key, data in tqdm(all_data.items(), desc=\"Label Generation\"):\n",
    "    try:\n",
    "        labels = generate_labels(data, window_size=WINDOW_SIZE)\n",
    "        all_labels[key] = labels\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating labels for {key}: {e}\")\n",
    "\n",
    "print(f\"✓ Generated labels for {len(all_labels)} configurations\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. EXPLORATORY DATA ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[4/10] Performing exploratory data analysis...\")\n",
    "\n",
    "# Combine all features and labels\n",
    "combined_features = []\n",
    "combined_labels = []\n",
    "\n",
    "for key in all_features.keys():\n",
    "    feat = all_features[key].copy()\n",
    "    lab = all_labels.get(key, pd.DataFrame())\n",
    "    \n",
    "    # Merge features with labels\n",
    "    if not lab.empty:\n",
    "        merged = feat.merge(lab, on=['vehicle_id', 'window'], how='left')\n",
    "    else:\n",
    "        merged = feat.copy()\n",
    "        merged['is_malicious'] = 0\n",
    "        merged['attack_type'] = 'benign'\n",
    "    \n",
    "    merged['is_malicious'] = merged['is_malicious'].fillna(0).astype(int)\n",
    "    merged['attack_type'] = merged['attack_type'].fillna('benign')\n",
    "    \n",
    "    combined_features.append(merged)\n",
    "\n",
    "# Create master dataframe\n",
    "master_df = pd.concat(combined_features, ignore_index=True)\n",
    "\n",
    "print(f\"\\n✓ Master dataset shape: {master_df.shape}\")\n",
    "print(f\"✓ Benign samples: {(master_df['is_malicious'] == 0).sum()}\")\n",
    "print(f\"✓ Malicious samples: {(master_df['is_malicious'] == 1).sum()}\")\n",
    "print(f\"\\nAttack type distribution:\")\n",
    "print(master_df['attack_type'].value_counts())\n",
    "\n",
    "# Save combined dataset\n",
    "master_df.to_csv(OUTPUT_DIR / \"master_features.csv\", index=False)\n",
    "print(f\"\\n✓ Saved master features to {OUTPUT_DIR / 'master_features.csv'}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. ML PIPELINE & TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[5/10] Training ML models...\")\n",
    "\n",
    "# Prepare features and labels\n",
    "feature_cols = [col for col in master_df.columns if col not in \n",
    "                ['vehicle_id', 'window', 'time_start', 'scenario', 'density', \n",
    "                 'run', 'is_malicious', 'attack_type']]\n",
    "\n",
    "X = master_df[feature_cols].values\n",
    "y_binary = master_df['is_malicious'].values\n",
    "y_multiclass = master_df['attack_type'].values\n",
    "\n",
    "# Encode multiclass labels\n",
    "le = LabelEncoder()\n",
    "y_multiclass_encoded = le.fit_transform(y_multiclass)\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Feature columns: {feature_cols}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train_bin, y_test_bin, y_train_multi, y_test_multi = train_test_split(\n",
    "    X, y_binary, y_multiclass_encoded, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_binary\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Binary classification model\n",
    "print(\"\\n--- Training Binary Classifier (Attack Detection) ---\")\n",
    "\n",
    "rf_binary = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_binary.fit(X_train_scaled, y_train_bin)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(rf_binary, X_train_scaled, y_train_bin, \n",
    "                            cv=CV_FOLDS, scoring='f1')\n",
    "print(f\"Cross-validation F1 scores: {cv_scores}\")\n",
    "print(f\"Mean CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "# Hyperparameter tuning (optional - can be time-consuming)\n",
    "TUNE_HYPERPARAMETERS = False  # Set to True to enable\n",
    "\n",
    "if TUNE_HYPERPARAMETERS:\n",
    "    print(\"\\n--- Hyperparameter Tuning ---\")\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 20, 30],\n",
    "        'min_samples_split': [5, 10, 20],\n",
    "        'min_samples_leaf': [2, 4, 8]\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1),\n",
    "        param_grid,\n",
    "        cv=3,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_scaled, y_train_bin)\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best F1 score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    rf_binary = grid_search.best_estimator_\n",
    "\n",
    "# Save model\n",
    "joblib.dump(rf_binary, MODEL_PATH)\n",
    "joblib.dump(scaler, OUTPUT_DIR / \"scaler.pkl\")\n",
    "print(f\"\\n✓ Saved model to {MODEL_PATH}\")\n",
    "\n",
    "# Multiclass classification\n",
    "print(\"\\n--- Training Multiclass Classifier (Attack Type Identification) ---\")\n",
    "\n",
    "rf_multiclass = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_multiclass.fit(X_train_scaled, y_train_multi)\n",
    "\n",
    "# ============================================================================\n",
    "# 7. MODEL EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[6/10] Evaluating models...\")\n",
    "\n",
    "# Binary classification predictions\n",
    "y_pred_bin = rf_binary.predict(X_test_scaled)\n",
    "y_pred_bin_proba = rf_binary.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Multiclass predictions\n",
    "y_pred_multi = rf_multiclass.predict(X_test_scaled)\n",
    "\n",
    "# Binary classification metrics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BINARY CLASSIFICATION RESULTS (Attack Detection)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "acc_bin = accuracy_score(y_test_bin, y_pred_bin)\n",
    "prec_bin = precision_score(y_test_bin, y_pred_bin)\n",
    "rec_bin = recall_score(y_test_bin, y_pred_bin)\n",
    "f1_bin = f1_score(y_test_bin, y_pred_bin)\n",
    "\n",
    "print(f\"Accuracy:  {acc_bin:.4f}\")\n",
    "print(f\"Precision: {prec_bin:.4f}\")\n",
    "print(f\"Recall:    {rec_bin:.4f}\")\n",
    "print(f\"F1-Score:  {f1_bin:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_bin, y_pred_bin, \n",
    "                          target_names=['Benign', 'Malicious']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm_bin = confusion_matrix(y_test_bin, y_pred_bin)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm_bin)\n",
    "\n",
    "# ROC-AUC\n",
    "roc_auc_bin = roc_auc_score(y_test_bin, y_pred_bin_proba)\n",
    "print(f\"\\nROC-AUC Score: {roc_auc_bin:.4f}\")\n",
    "\n",
    "# Multiclass metrics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MULTICLASS CLASSIFICATION RESULTS (Attack Type)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "acc_multi = accuracy_score(y_test_multi, y_pred_multi)\n",
    "prec_multi = precision_score(y_test_multi, y_pred_multi, average='weighted')\n",
    "rec_multi = recall_score(y_test_multi, y_pred_multi, average='weighted')\n",
    "f1_multi = f1_score(y_test_multi, y_pred_multi, average='weighted')\n",
    "\n",
    "print(f\"Accuracy:  {acc_multi:.4f}\")\n",
    "print(f\"Precision: {prec_multi:.4f}\")\n",
    "print(f\"Recall:    {rec_multi:.4f}\")\n",
    "print(f\"F1-Score:  {f1_multi:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_multi, y_pred_multi, \n",
    "                          target_names=le.classes_))\n",
    "\n",
    "# Save metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Model': ['Binary', 'Multiclass'],\n",
    "    'Accuracy': [acc_bin, acc_multi],\n",
    "    'Precision': [prec_bin, prec_multi],\n",
    "    'Recall': [rec_bin, rec_multi],\n",
    "    'F1-Score': [f1_bin, f1_multi]\n",
    "})\n",
    "\n",
    "metrics_df.to_csv(OUTPUT_DIR / \"ml_metrics.csv\", index=False)\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_binary.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "feature_importance.to_csv(OUTPUT_DIR / \"feature_importance.csv\", index=False)\n",
    "\n",
    "print(\"\\n✓ Top 10 Important Features:\")\n",
    "print(feature_importance.head(10).to_string())\n",
    "\n",
    "# ============================================================================\n",
    "# 8. TRUST SCORE COMPUTATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[7/10] Computing trust scores...\")\n",
    "\n",
    "def compute_trust_scores(features_df, alpha=0.7):\n",
    "    \"\"\"\n",
    "    Compute trust scores using exponential smoothing.\n",
    "    \n",
    "    trust_t = α × consistency_t + (1-α) × trust_{t-1}\n",
    "    \n",
    "    Args:\n",
    "        features_df: DataFrame with features\n",
    "        alpha: Smoothing parameter\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Trust scores per vehicle over time\n",
    "    \"\"\"\n",
    "    trust_scores = []\n",
    "    \n",
    "    for vehicle_id in features_df['vehicle_id'].unique():\n",
    "        vehicle_data = features_df[features_df['vehicle_id'] == vehicle_id].sort_values('window')\n",
    "        \n",
    "        trust = 1.0  # Initial trust\n",
    "        \n",
    "        for _, row in vehicle_data.iterrows():\n",
    "            # Compute consistency based on multiple factors\n",
    "            consistency = 0.0\n",
    "            n_factors = 0\n",
    "            \n",
    "            # Factor 1: Neighbor consistency (more neighbors = more consistent)\n",
    "            if 'unique_neighbors' in row and row['unique_neighbors'] > 0:\n",
    "                consistency += min(row['unique_neighbors'] / 10.0, 1.0)\n",
    "                n_factors += 1\n",
    "            \n",
    "            # Factor 2: RSSI stability (low variance = more stable)\n",
    "            if 'rssi_variance' in row and row['rssi_variance'] >= 0:\n",
    "                rssi_stability = 1.0 / (1.0 + row['rssi_variance'] / 100.0)\n",
    "                consistency += rssi_stability\n",
    "                n_factors += 1\n",
    "            \n",
    "            # Factor 3: Message plausibility (speed variations)\n",
    "            if 'speed_std' in row and row['speed_std'] >= 0:\n",
    "                speed_plausibility = 1.0 / (1.0 + row['speed_std'] / 10.0)\n",
    "                consistency += speed_plausibility\n",
    "                n_factors += 1\n",
    "            \n",
    "            # Average consistency\n",
    "            if n_factors > 0:\n",
    "                consistency /= n_factors\n",
    "            else:\n",
    "                consistency = 0.5  # Neutral\n",
    "            \n",
    "            # Update trust with exponential smoothing\n",
    "            trust = alpha * consistency + (1 - alpha) * trust\n",
    "            \n",
    "            trust_scores.append({\n",
    "                'vehicle_id': vehicle_id,\n",
    "                'window': row['window'],\n",
    "                'time': row['window'] * WINDOW_SIZE,\n",
    "                'trust_score': trust,\n",
    "                'consistency': consistency\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(trust_scores)\n",
    "\n",
    "# Compute trust scores for all scenarios\n",
    "trust_scores_all = {}\n",
    "\n",
    "for key, features in all_features.items():\n",
    "    if not features.empty:\n",
    "        trust = compute_trust_scores(features, alpha=TRUST_ALPHA)\n",
    "        trust['scenario'] = features['scenario'].iloc[0]\n",
    "        trust['density'] = features['density'].iloc[0]\n",
    "        trust_scores_all[key] = trust\n",
    "\n",
    "# Combine all trust scores\n",
    "combined_trust = pd.concat(trust_scores_all.values(), ignore_index=True)\n",
    "combined_trust.to_csv(OUTPUT_DIR / \"trust_scores.csv\", index=False)\n",
    "\n",
    "print(f\"✓ Computed trust scores for {len(trust_scores_all)} configurations\")\n",
    "print(f\"✓ Mean trust score: {combined_trust['trust_score'].mean():.4f}\")\n",
    "print(f\"✓ Trust score std: {combined_trust['trust_score'].std():.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 9. VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[8/10] Creating visualizations...\")\n",
    "\n",
    "# 1. ROC Curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "fpr, tpr, _ = roc_curve(y_test_bin, y_pred_bin_proba)\n",
    "plt.plot(fpr, tpr, label=f'Binary Classifier (AUC = {roc_auc_bin:.3f})', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve - Attack Detection', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"roc_curve.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 2. Confusion Matrix - Binary\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_bin, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Benign', 'Malicious'],\n",
    "            yticklabels=['Benign', 'Malicious'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Binary Classification', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"confusion_matrix_binary.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 3. Confusion Matrix - Multiclass\n",
    "cm_multi = confusion_matrix(y_test_multi, y_pred_multi)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_multi, annot=True, fmt='d', cmap='YlOrRd',\n",
    "            xticklabels=le.classes_,\n",
    "            yticklabels=le.classes_,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Attack Type Classification', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"confusion_matrix_multiclass.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 4. Feature Importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_n = 15\n",
    "top_features = feature_importance.head(top_n)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.title(f'Top {top_n} Most Important Features', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"feature_importance.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 5. Trust Score Over Time (sample vehicles)\n",
    "plt.figure(figsize=(14, 6))\n",
    "sample_vehicles = combined_trust['vehicle_id'].unique()[:5]\n",
    "for vehicle_id in sample_vehicles:\n",
    "    vehicle_trust = combined_trust[combined_trust['vehicle_id'] == vehicle_id]\n",
    "    plt.plot(vehicle_trust['time'], vehicle_trust['trust_score'], \n",
    "             marker='o', markersize=3, label=f'Vehicle {vehicle_id}', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Time (s)', fontsize=12)\n",
    "plt.ylabel('Trust Score', fontsize=12)\n",
    "plt.title('Trust Score Evolution Over Time (Sample Vehicles)', fontsize=14, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.ylim([0, 1.05])\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"trust_scores_over_time.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 6. PDR Comparison Across Scenarios\n",
    "plt.figure(figsize=(12, 6))\n",
    "pdr_data = network_stats_df.pivot_table(values='pdr', index='density', columns='scenario')\n",
    "pdr_data.plot(kind='bar', ax=plt.gca(), width=0.7)\n",
    "plt.xlabel('Vehicle Density (vehicles/km)', fontsize=12)\n",
    "plt.ylabel('Packet Delivery Ratio', fontsize=12)\n",
    "plt.title('PDR Comparison Across Scenarios', fontsize=14, fontweight='bold')\n",
    "plt.legend(title='Scenario', fontsize=11)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"pdr_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 7. Attack Detection Performance by Scenario\n",
    "scenario_performance = []\n",
    "for scenario in SCENARIOS:\n",
    "    scenario_mask = master_df['scenario'] == scenario\n",
    "    if scenario_mask.sum() > 0:\n",
    "        X_scenario = master_df[scenario_mask][feature_cols].values\n",
    "        y_scenario = master_df[scenario_mask]['is_malicious'].values\n",
    "        \n",
    "        if len(y_scenario) > 0 and y_scenario.sum() > 0:\n",
    "            X_scenario_scaled = scaler.transform(X_scenario)\n",
    "            y_pred_scenario = rf_binary.predict(X_scenario_scaled)\n",
    "            \n",
    "            scenario_performance.append({\n",
    "                'scenario': scenario,\n",
    "                'accuracy': accuracy_score(y_scenario, y_pred_scenario),\n",
    "                'precision': precision_score(y_scenario, y_pred_scenario, zero_division=0),\n",
    "                'recall': recall_score(y_scenario, y_pred_scenario, zero_division=0),\n",
    "                'f1_score': f1_score(y_scenario, y_pred_scenario, zero_division=0)\n",
    "            })\n",
    "\n",
    "if scenario_performance:\n",
    "    perf_df = pd.DataFrame(scenario_performance)\n",
    "    perf_df_melted = perf_df.melt(id_vars='scenario', var_name='metric', value_name='score')\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=perf_df_melted, x='scenario', y='score', hue='metric')\n",
    "    plt.xlabel('Scenario', fontsize=12)\n",
    "    plt.ylabel('Score', fontsize=12)\n",
    "    plt.title('ML Performance by Scenario', fontsize=14, fontweight='bold')\n",
    "    plt.legend(title='Metric', fontsize=10)\n",
    "    plt.ylim([0, 1.05])\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / \"performance_by_scenario.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    perf_df.to_csv(OUTPUT_DIR / \"performance_by_scenario.csv\", index=False)\n",
    "\n",
    "print(f\"✓ Saved all visualizations to {FIG_DIR}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 10. RESULTS EXPORT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[9/10] Exporting results...\")\n",
    "\n",
    "# Generate LaTeX table for metrics\n",
    "def generate_latex_table(df, caption, label):\n",
    "    \"\"\"Generate LaTeX table from DataFrame.\"\"\"\n",
    "    latex = df.to_latex(index=False, float_format=\"%.4f\", \n",
    "                        caption=caption, label=label)\n",
    "    return latex\n",
    "\n",
    "# Binary classification metrics LaTeX\n",
    "latex_binary = generate_latex_table(\n",
    "    pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n",
    "        'Score': [acc_bin, prec_bin, rec_bin, f1_bin, roc_auc_bin]\n",
    "    }),\n",
    "    caption=\"Binary Classification Performance (Attack Detection)\",\n",
    "    label=\"tab:binary_metrics\"\n",
    ")\n",
    "\n",
    "with open(OUTPUT_DIR / \"binary_metrics.tex\", 'w') as f:\n",
    "    f.write(latex_binary)\n",
    "\n",
    "# Multiclass classification metrics LaTeX\n",
    "latex_multi = generate_latex_table(\n",
    "    pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "        'Score': [acc_multi, prec_multi, rec_multi, f1_multi]\n",
    "    }),\n",
    "    caption=\"Multiclass Classification Performance (Attack Type Identification)\",\n",
    "    label=\"tab:multiclass_metrics\"\n",
    ")\n",
    "\n",
    "with open(OUTPUT_DIR / \"multiclass_metrics.tex\", 'w') as f:\n",
    "    f.write(latex_multi)\n",
    "\n",
    "# Network statistics LaTeX\n",
    "latex_network = generate_latex_table(\n",
    "    network_stats_df,\n",
    "    caption=\"Network Statistics Summary\",\n",
    "    label=\"tab:network_stats\"\n",
    ")\n",
    "\n",
    "with open(OUTPUT_DIR / \"network_stats.tex\", 'w') as f:\n",
    "    f.write(latex_network)\n",
    "\n",
    "print(\"✓ Exported LaTeX tables\")\n",
    "\n",
    "# Save comprehensive summary\n",
    "summary = {\n",
    "    'timestamp': pd.Timestamp.now(),\n",
    "    'total_samples': len(master_df),\n",
    "    'benign_samples': (master_df['is_malicious'] == 0).sum(),\n",
    "    'malicious_samples': (master_df['is_malicious'] == 1).sum(),\n",
    "    'num_features': len(feature_cols),\n",
    "    'binary_accuracy': acc_bin,\n",
    "    'binary_precision': prec_bin,\n",
    "    'binary_recall': rec_bin,\n",
    "    'binary_f1': f1_bin,\n",
    "    'binary_roc_auc': roc_auc_bin,\n",
    "    'multiclass_accuracy': acc_multi,\n",
    "    'multiclass_f1': f1_multi,\n",
    "    'cv_f1_mean': cv_scores.mean(),\n",
    "    'cv_f1_std': cv_scores.std()\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_df.to_csv(OUTPUT_DIR / \"summary.csv\", index=False)\n",
    "\n",
    "with open(OUTPUT_DIR / \"summary.txt\", 'w') as f:\n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "    f.write(\"V2X ML ANALYSIS SUMMARY\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    for key, value in summary.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "print(\"✓ Saved comprehensive summary\")\n",
    "\n",
    "# ============================================================================\n",
    "# 11. STATISTICAL SIGNIFICANCE TESTING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[10/10] Performing statistical significance tests...\")\n",
    "\n",
    "# Compare performance across densities\n",
    "density_performance = []\n",
    "\n",
    "for density in DENSITIES:\n",
    "    density_mask = master_df['density'] == density\n",
    "    if density_mask.sum() > 0:\n",
    "        X_density = master_df[density_mask][feature_cols].values\n",
    "        y_density = master_df[density_mask]['is_malicious'].values\n",
    "        \n",
    "        if len(y_density) > 0 and y_density.sum() > 0:\n",
    "            X_density_scaled = scaler.transform(X_density)\n",
    "            y_pred_density = rf_binary.predict(X_density_scaled)\n",
    "            \n",
    "            density_performance.append({\n",
    "                'density': density,\n",
    "                'f1_score': f1_score(y_density, y_pred_density, zero_division=0)\n",
    "            })\n",
    "\n",
    "if len(density_performance) >= 2:\n",
    "    print(\"\\nF1-Score by Density:\")\n",
    "    for item in density_performance:\n",
    "        print(f\"  Density {item['density']}: {item['f1_score']:.4f}\")\n",
    "    \n",
    "    # Note: For proper statistical testing, would need multiple runs\n",
    "    print(\"\\n(Note: Statistical significance testing requires multiple runs with different seeds)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nOutputs saved to:\")\n",
    "print(f\"  - Figures: {FIG_DIR}\")\n",
    "print(f\"  - Data/Metrics: {OUTPUT_DIR}\")\n",
    "print(f\"  - Model: {MODEL_PATH}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# 12. SUMMARY & NEXT STEPS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\"\"\n",
    "1. Dataset Statistics:\n",
    "   - Total samples: {len(master_df):,}\n",
    "   - Benign: {(master_df['is_malicious'] == 0).sum():,} ({(master_df['is_malicious'] == 0).sum()/len(master_df)*100:.1f}%)\n",
    "   - Malicious: {(master_df['is_malicious'] == 1).sum():,} ({(master_df['is_malicious'] == 1).sum()/len(master_df)*100:.1f}%)\n",
    "\n",
    "2. Binary Classification (Attack Detection):\n",
    "   - Accuracy: {acc_bin:.4f}\n",
    "   - Precision: {prec_bin:.4f}\n",
    "   - Recall: {rec_bin:.4f}\n",
    "   - F1-Score: {f1_bin:.4f}\n",
    "   - ROC-AUC: {roc_auc_bin:.4f}\n",
    "\n",
    "3. Multiclass Classification (Attack Type):\n",
    "   - Accuracy: {acc_multi:.4f}\n",
    "   - F1-Score: {f1_multi:.4f}\n",
    "\n",
    "4. Cross-Validation:\n",
    "   - Mean F1: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\n",
    "\n",
    "5. Top 5 Most Important Features:\n",
    "\"\"\")\n",
    "\n",
    "for idx, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"   - {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "6. Trust Scores:\n",
    "   - Mean: {combined_trust['trust_score'].mean():.4f}\n",
    "   - Std: {combined_trust['trust_score'].std():.4f}\n",
    "\n",
    "7. Network Performance (Average PDR):\n",
    "   - Highway: {network_stats_df[network_stats_df['scenario']=='highway']['pdr'].mean():.4f}\n",
    "   - Mixed: {network_stats_df[network_stats_df['scenario']=='mixed']['pdr'].mean():.4f}\n",
    "   - Urban: {network_stats_df[network_stats_df['scenario']=='urban']['pdr'].mean():.4f}\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NEXT STEPS & EXTENSIONS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "1. Multi-Run Analysis:\n",
    "   - Extend RUNS list to include ['run-1', 'run-2', 'run-3', ...]\n",
    "   - Compute mean and std of metrics across runs\n",
    "   - Perform statistical significance testing\n",
    "\n",
    "2. Hyperparameter Optimization:\n",
    "   - Set TUNE_HYPERPARAMETERS = True for GridSearchCV\n",
    "   - Experiment with different algorithms (XGBoost, SVM, Neural Networks)\n",
    "\n",
    "3. Advanced Features:\n",
    "   - Add temporal dependencies (LSTM/RNN for sequence modeling)\n",
    "   - Include spatial features (vehicle positions, trajectories)\n",
    "   - Experiment with ensemble methods\n",
    "\n",
    "4. Real-time Deployment:\n",
    "   - Create streaming pipeline for online learning\n",
    "   - Implement incremental model updates\n",
    "   - Deploy with model monitoring\n",
    "\n",
    "5. Explainability:\n",
    "   - Add SHAP values for feature importance\n",
    "   - Implement LIME for local explanations\n",
    "   - Create decision path visualizations\n",
    "\n",
    "6. Class Imbalance:\n",
    "   - Experiment with SMOTE/ADASYN\n",
    "   - Adjust class weights\n",
    "   - Try cost-sensitive learning\n",
    "\n",
    "7. Attack-Specific Models:\n",
    "   - Train separate models for each attack type\n",
    "   - Implement hierarchical classification\n",
    "   - Build attack-specific trust mechanisms\n",
    "\n",
    "8. Performance Optimization:\n",
    "   - Profile code for bottlenecks\n",
    "   - Parallelize feature extraction\n",
    "   - Use Dask for large-scale data\n",
    "\n",
    "To use this notebook:\n",
    "1. Update ROOT_DIR to your data location\n",
    "2. Run all cells (Cell → Run All)\n",
    "3. Check outputs in {OUTPUT_DIR}\n",
    "4. Visualizations in {FIG_DIR}\n",
    "5. Trained model at {MODEL_PATH}\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
